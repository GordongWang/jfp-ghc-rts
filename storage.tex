\section{Storage}

An essential component of a runtime system for any high-level
programming language is the garbage collector, which is responsible
identifying and reclaiming memory from objects which are no longer in
use by the program.  When it comes to a garbage collector,
\emph{efficiency} is the order of the day: the speed of the garbage
collector affects the performance of all programs running on the
runtime, and thus the GHC runtime devotes a substantial portion of its
complexity budget to a fast garbage collector.  What do we need for
a fast garbage collector?

\subsection{Blocks}

The very first consideration is a low-level one: ``Where is the memory coming
from?''  The runtime can request memory from the operating system via
\verb|malloc|, but how much should it request, and how should it be
used?  A simple design is to request a large, contiguous block of memory
and use it as the heap, letting the garbage collector manage objects allocated
within it.  However, this scheme is fairly inflexible: when one of these heaps
runs out of memory, we need to double the size of the heap and copy all of the
old data into the new memory.  Picking the initial sizes of heaps can be an exercise
in fiddling with black magic tuning parameters.

A more flexible scheme is a \emph{block-structured heap}~\cite{maclisp,Dybvig94don'tstop,Marlow:2008:PGG:1375634.1375637}.
The basic idea is to divide the heap into fixed-size $B$-byte blocks,
where $B$ is a power of two: blocks are then linked together in order to
provide memory for the heap.\footnote{GHC uses 4kb blocks, but this is an easily
adjustable constant.}  These blocks need not be contiguous: thus, if your
heap runs out of space, instead of having to double the size of your heap,
you can simply chain a few more blocks onto it.  There are some other benefits as well:

\begin{enumerate}
    \item Large objects (e.g. close to block size) do not have to be copied from one region to
        another; instead, the block they reside in can be relinked from
        one region to another.\footnote{Of course, this requires \emph{only}
        one object to live in a block, which can result in fragmentation.
        However, empirically this does not seem to have caused much of a problem.}
    \item Blocks make it easy to provide heap memory in contexts where it is
        not possible to perform garbage collection.  As an example, consider
        the GMP arbitrary-precision arithmetic library.  This C code requires
        the ability to allocate memory while performing internal computation.
        However, if the heap runs out of memory, what can you do?  If the heap
        were contiguous, you would now need to carry out a GC to free up some memory
        (or get a larger heap); but this would require us to halt the C code
        (arbitrarily deep in some internal computation) while simultaneously being
        able to identify all pointers to the heap that it may be holding.  Whereas
        in a block-structured heap, we can simply grab a new block and defer the GC
        until later.
    \item Free memory can be recycled quickly, since a free block can be quickly
        reused somewhere else.
\end{enumerate}

One reason why this scheme works so well is that most objects on the
heap are much smaller than the block size; handling these cases is very
simple.  When an object is larger than a block size, it needs to be
placed into a \emph{block group} of contiguous blocks---which in turn
need to be handled with some care to avoid fragmentation.  The blocks
themselves are provided by the operating system in large units called
\emph{megablocks}.\footnote{1Mb in size, in the current implementation.}

Finally, each block is associated with a \emph{block descriptor}, which
contains information about the block such as what generation it belongs to, how full it is, what block
group it is part of, etc.  An obvious place to put the block descriptor
is at the beginning of a block, but this runs into problems when the block
is the member of a block group (the memory must be contiguous!)
Thus, the descriptors of blocks of a megablock are instead organized together
in the first block of a megablock; some care is taken to ensure that the
runtime can efficiently compute the block descriptor of any given block.

\subsection{Memory layout}

Before we can discuss the garbage collector proper, we have to describe
the layout of the data that is to be garbage collector.  GHC has a uniform
representation for objects on the heap with a \emph{header}, which indicates
what kind of object the data is, and a \emph{payload}, which contains
the actual data for an object (e.g. free variables for function values
and thunks, or fields for data values).  The header points to an
\emph{info table}, which provides more information about what kind of
object the closure is, what code is associated with the object and what
the layout of the payload is (e.g. what fields are pointers.)

The presence of info tables makes it easy for the garbage collector to
determine what other closures on the heap an object may reference, as it
says which fields in the payload are pointers.  Essentially everything
that the GC touches has an info table, including the stack frames (each
block of compiled code receives its own info table.)  In particular,
this makes calculating the \emph{roots} (base objects which are always
considered reachable) of the application much simpler: at the beginning
of any block of code, the info table and registers (which known and
saved by the code itself) constitute all of the pointers in use,
allowing us to accurately perform GC.

There are many possible methods by which objects on the heap can be
represented (for example, in place of info tables, pointer tagging can
be used to distinguish non-pointers from pointers).  However, in order
to support lazy evaluation, headers have
another important function, which is that they double as pointers to the
\emph{entry code} responsible for evaluating a thunk.  The ability to \emph{replace}
one header with another and have the behavior of a thunk change
correspondingly is tremendously useful.  The issues are discussed in Section~\ref{sec:lazy}.

\subsection{Generational garbage collection}

The next question you might ask is, ``What kind of garbage collector
should I use?''  By default, GHC uses a generational copying collector.

A \emph{generational collector} divides the heap into
\emph{generations}, where generations are numbered with the zero being
the youngest.  Objects are allocated into the youngest generation, which
has garbage collection performed on it whenever it runs out of memory.
Surviving objects are \emph{promoted} to the next generation, which is
collected less frequently, and so forth.  In a \emph{copying collector}, this
promotion occurs by simply copying the object into the \emph{to-space},
a process called \emph{evacuation}.  Evacuated objects are subsequently
\emph{scavenged} to find what objects they refer to, so they can be evacuated as well.

%   \SM{We probably shouldn't use the terms ``evacuate'' and
%     ``scavenge'', since I think the GHC GC uses them inconsistently with
%     the literature. (TODO: check).}

%   \Red{EZY: I think, probably, we should just use these terms, since the literature
%   doesn't really have a snappy name for "copy to to-space" and "scan for pointers",
%   and I use these terms a lot later on.}

The efficacy of generational collection hinges on the ``generational
hypothesis'', which states that data that has been recently allocated is
the most likely to die and become unreachable.  This tends to be
particularly true of functional programs, which encourage the use of short-lived
intermediate data structures to help structure computation.  In fact,
functional programs allocate so much memory that it makes sense not to
immediately promote data, since objects may not have had sufficient
chance to die by the time of the first GC.  Thus, GHC further implements
an \emph{aging} scheme, where reachable objects in generation 0 are not
immediately promoted to generation 1; instead, they are aged and
promoted the next GC cycle.\footnote{When objects are only aged once, an
equivalent way of stating this scheme is that generation 0 is split into
two generations, but we never garbage collect just the younger
generation, we always collect both on a minor collection.  This is in fact
how GHC implements aging.}

Use of a copying collector has other benefits for allocating heavy
workloads.  In particular, copying collection ensures that free memory
is contiguous, which allows for extremely efficient memory allocation
using a \emph{bump allocator}---so named because a heap allocation
simply involves bumping up the free space pointer. Additionally, while
copying collectors are often criticized for wasting half of their
allocated memory to maintain the two spaces for copying, a block
structured heap can immediately reuse blocks in the from-space as soon
as they are fully evacuated.

\subsubsection{Mutability in the GC}

The primary complication when implementing a generational garbage
collector is the treatment of mutable references.  When a heap is
immutable, pointers in the young generation can only ever point into
older generations; thus, to discover all reachable objects when
collecting an old generation, it suffices to simply collect all younger
generations when performing an (infrequent) collection of an older
generation.  However, if objects in the old generation are mutable, they
may point back into the young generation, in which case we need to know
that those objects are reachable even when the only references to them
are in the old generation (which we would like to avoid collecting).

The solution to this problem is to apply a \emph{GC write barrier}
(sometimes confusingly referred to as a \emph{write barrier}) to memory
writes, adding the mutated object to a \emph{remembered set} which is
also considered a root for garbage collection.  Now the GC cannot
accidentally conclude an object pointed to by a mutable reference in an
old generation is dead: it will discover its reachability through the
remembered set.  However, this scheme is costly in two ways: first, all
mutation must pay the overhead of adding the object to the remembered
set, and second, as the remembered set increases in size, the amount of
heap that must be traversed during a minor collection also increases.

In the first case, GHC keeps track of mutation per object, spending a
single memory write to add a mutated object to a mutable list.  This
design lies in a continuum of precision: one could increase the
precision of the remembered set by only adding mutable fields (rather
than objects), or one could decrease the precision by only tracking
\emph{cards} (i.e. portions of the heap) at a larger granularity.
Increased precision increases the overhead of the mutable list but
reduces the amount of extra work the GC needs to perform, while reduced
precision makes mutation more efficient but leads to slower minor
collections.  We think that mutation per object is a good balance: mutation
is not prevalant enough in functional code that coarse-grained card making
buys much, and most mutable objects in Haskell are quite small, with only
one or two fields.\footnote{However, this assumption has caused the GHC runtime
some grief, e.g. in the case of mutable arrays of pointers, which we used to
scan the entirety.  Today, we have a card-marking scheme to permit mutable
arrays to be efficiently GC'd.}

In the second case, GHC can take advantage of an interesting property of
lazy functional programs: thunks are only ever mutated once, in order to
update them with their fully evaluated values---they are immutable
afterwards.  Thus, we can immediately eliminate an updated thunk from
the mutable list by \emph{eagerly promoting} the data the updated thunk
points to into the same generation as the thunk itself.  Since the thunk
is immutable, this data is guaranteed not to be GC'd until the thunk
itself is GC'd as well.  This leads to an interesting constraint on how
garbage collection proceeds: we must collect older generations first, so
that objects we may want to promote have not been evacuated yet.
Because an already evacuated object may have forwarding pointers
pointing to it, it cannot be evacuated again within the same GC.

%   This procedure is divided into two steps: \emph{evacuation},
%   which performs the copy, replacing the old object with a forwarding
%   pointer to the new object, and \emph{scavenging}, which scans all
%   objects that are newly copied in to-space and evacuates their fields.

% \SM{You captured all the issues here well. Nice job!}

\subsection{Parallel garbage collection}

A generational garbage collector offers quite good performance, but
there is still the question, ``Is it fast enough?''  One avenue for
speeding up the garbage collector when multiple cores are available is
to perform \emph{parallel garbage collection}, having multiple threads
traverse the heap in parallel.  Note the distinction from
\emph{concurrent collection}, where the GC runs concurrently with user
code which is mutating values on the heap.  GHC implements parallel
collection~\cite{Marlow:2008:PGG:1375634.1375637} but not concurrent
collection: concurrent collection requires synchronization between the
GC and the mutator and consequently is more complex.  However, we have
experimented with a form of concurrent collection in which individual
cores have local heaps that can be collected independently of activity
on the other cores~\cite{local-heaps}.

There are two primary technical challenges that accompany building a
parallel garbage collector.  The first is how to divide the GC work
among the threads, the second is how to synchronize when two GC threads
attempt to evacuate the same object.

GHC overcomes the first challenge by utilizing the block structure of
the heap.  In particular, a block in the to-space of a garbage
collection constitutes a unit of work: a thread can either claim the
block to scavenge for itself, or the block can be transferred to another
thread to process.  Once blocks are chosen as the basic unit of work,
there are a variety of mechanisms by which work can be shared: blocks
can be statically assigned to GC threads with no runtime load balancing,
blocks can be taken from a global queue which provides blocks to all
threads, or a hybrid solution can have threads have local queues, but
permit other threads to steal work from other queues when they are idle
via a \emph{work-stealing queue
structure}.~\cite{Arora:1998:TSM:277651.277678}  GHC originally
implemented a single global queue, but we have since switched
work-stealing queues because they have much better data locality, as
processors prefer to take work from their local queues before stealing
work from others.  In fact, we don't want to do any load-balancing on
minor collections, because it ruins locality by shipping work off to
another core when the data is likely \emph{already} in the cache of the
original core.~\cite{Marlow2009}

The second challenge reflects the primary cost of parallel GC, which is
the extra synchronization overhead any parallel scheme will impose.  In
particular, we must prevent the GC from duplicating mutable objects when
multiple threads attempt to evacuate the same object by synchronizing
the object (by either locking it pessimistically or compare-and-swapping
optimistically).  This synchronization is expensive; fortunately, there
is a wonderful benefit for immutable objects: they require no
synchronization, because it is safe to have multiple copies of an
immutable data structure!  Eliminating the locks in these cases accounts
for a 20-30\% speedup, which is nothing to sneeze at.

One important parameter which must be set properly is the size of the
young generation, i.e. the nursery.  If the nursery is too small, then
we will need to perform minor garbage collections too frequently.  But
if it is too large, then cores will generate a lot of memory traffic
getting data that is not in their cache.  In general, memory bandwidth
is the bottleneck when multiple cores are allocating quickly; thus,
having the nursery be the size of the cache is generally the best setting.
\Red{But perhaps see the discussion at } \verb|http://donsbot.wordpress.com/2010/07/05/ghc-gc-tune-tuning-haskell-gc-settings-for-fun-and-profit/|

%   In general, writing a parallel garbage collector is tricky business:
%   it's easy to wipe out performance gains by accident.  Of particular delicacy
%   is the cache behavior

%   \SM{I think we don't need to talk about the two types of locking (I
%     don't think there's a deep reason to choose one over the other, just
%     minor engineering or performance issues).  What's more important to
%     mention here is that we can avoid locking immutable objects---a win
%     for immutability.}

%   \SM{I think we should talk about the whole idea of having small
%     nurseries that fit in the cache, since this is the only way to scale
%     GC when you have multiple cores allocating fast (they run into the
%     memory bandwidth otherwise).  And related to this is the idea that
%     we don't want to do any load-balancing when we do G0 collections,
%     because that ruins the cache.  Some measurements in the ICFP'09
%     paper.}

\subsection{Summary}

\SM{Not sure whether going into this much detail is helpful to the
  average reader.  I'm prepared to be persuaded otherwise.}

We conclude by sketching the overall operation of GHC's parallel,
generational, block-structured garbage collector, with all of the
features that we have described thus far.

The main garbage collection function
\verb|GarbageCollect| in \verb|rts/sm/GC.c| proceeds (after all user
execution is halted) as follows:

\begin{enumerate}
    \item Prepare all of the collected generations by moving their blocks
        into the from-space and throwing out their mutable lists (recall the remembered
        set is only necessary when the generation is not being collected.)
        The blocks themselves indicate what generation live objects in them should be promoted to.
    \item Wakeup the GC threads, initializing them with eager promotion enabled.
    \item \emph{Evacuate} the roots of application (including the mutable lists of all older generations), giving work to the main GC thread to do.
    \item In a loop, each thread:
        \begin{enumerate}
            \item Look for local blocks to \emph{scavenge} (e.g. if the thread recently evacuated some objects which it hasn't scavenged), starting with blocks from the oldest generation.
            \item Try to steal blocks from another thread (at the very beginning of a GC, idle GC threads are likely to steal work from the main thread, if they didn't have any work to begin with).
            \item Halt execution, but while there are still GC threads running, poll to see whether or not there is any work to do.
        \end{enumerate}
    \item Cleanup after the GC, which includes running finalizers, returning memory to the operating system, resurrecting threads~\XXX, etc.
\end{enumerate}

The \emph{evacuate} function \verb|evacuate| in \verb|rts/sm/Evac.c|
takes a pointer to an object and attempts to copy it into a destination
generation to-space, as specified by the block it resides in (or the generation that
it needs to be promoted to, if eager promotion is enabled).  Before doing so,
it performs the following checks:

\begin{enumerate}
    \item Is the object heap allocated? (If not, it is handled specially.)
    \item Was the object already evacuated (e.g. the pointer already points
        to a to-space, or the object is a forwarding pointer)?  If it
        was, and to a generation which is younger than the intended
        target, then it reports the evacuation as failed (so scavenge
        can add a mutable reference pointing to the object to a mutable
        list, etc.)
\end{enumerate}

After the copy, the original is overwritten with a forwarding pointer.
If the object in question is mutable, this is done atomically with a
compare-and-swap to avoid races between two threads evacuating the same
object.

The \emph{scavenge} function \verb|scavenge_block| in
\verb|rts/sm/Scav.c| walks a pointer down the provided block (filled in
previously by evacuate), reading the info table in order to determine
what kind of object it is.  It evacuates the fields of the object,
temporarily turning off eager promotion if the object is mutable.  If
evacuation is unsuccessful for the field of a mutable object, it must be
added back to the mutable list.  When the block is finished being
scavenged, it gets pushed to the list of completed blocks.  The block
that is scavenged can be thought of as the ``pending work queue''; this
optimization was first suggested as part of Cheney's algorithm and
avoids the need for an explicit queue of pending objects to scavenge.

\subsection{Further reading}

While we have discussed many of the most important features of GHC's
garbage collector, there remain many other features we have not
discussed here.  These include:

\begin{itemize}
    \item an implementation of a compacting collector, \Red{no docs about this!}
    \item support for weak pointers and finalizers,~\cite{PeytonJones:1999:SSM:647978.743369} \Red{We might actually want to talk about this, it is probably one of the more voodoo-y parts of the system} and
    \item garbage collection of static objects.
\end{itemize}

We have also omitted many details about the features we have discussed.
For a good account of the block-structured parallel garbage collector,
please see~\cite{Marlow:2008:PGG:1375634.1375637}; however, since the
paper was published the default locking and load balancing schemes for
the parallel GC have changed, and we have implemented the improvement
described in Section 7.1.  Additionally, the GHC Commentary~\cite{ghc-gc-commentary} has good
articles for technically inclined GHC hackers on a variety of issues we
have discussed here, including eager promotion, remembered
sets \Red{ete etc}
