THREADS
-------

OK, this works OK, but what about concurrent Haskell?  Concurrent
Haskell has a few parameters:
    1. Processes
    2. Atomically mutable state, i.e. mechanisms for communicating between processes

Naive idea: give each process its own OS thread (one-to-one).  Better idea: multiple
lightweight threads mapped onto a single operating system thread (multiplexed).  This is
cheap because we don't have to interact with expensive OS thread facilities.

Let us presuppose that we want multiplexing.  We now have a gap: "OS threads" ~~~~ "Haskell threads", this has consequences.

First, decouple Haskell thread representation from OS. Stacks are no longer OS stacks, thread now needs to store more information (e.g. the "thread-local" errno number, what the thread ID is, what the thread's run status is, what capability the thread is running on--this bit is important for IPC).  So turn this into first-class objects and store them on the heap.  (Implementation detail: one monolithic stack or stack chunks? GHC's changed its mind over the years, now we use stack chunks).  Performance note: stacks have dirty bits, so we don't have to repeatedly scan them when threads haven't run.

Second, we need to map Haskell threads onto operating system threads, so they actually can run.

Idea: take your operating system threads (call it a Haskell Execution Context or Capability), and give them a run queue of Haskell threads (a simple doubly linked list--this means TSOs are MUTABLE with POINTERS to STATE--recall GC injunctions).  The loop goes:

    while(1) {
        tso = popRunQueue(cap)
        result = StgRun(tso)
        case result of
            out of heap -> re-enqueue tso; call GC
                Note about large blocks: need to add the large block to the nursery which is per CAPABILITY, try to run the thread immediately afterwards
            out of stack -> enlarge stack; re-enqueue tso
            time expired -> put tso on end of queue
                Note about how the context switch timer works: it sets a variable (Hp) which then causes Haskell to boot out of the loop and back to the scheduler
            finished -> continue
    }

Notice: Garbage collecting processes: the RUN QUEUE is the root!  (If the system is deadlocked, a GC can identify groups of deadlocked threads... but see later about resurrecting threads with exceptions--this requires us to talk about weak pointers)

Note: garbage collection requires ALL capabilities to be acquired, since we do NOT have a concurrent collector.  Do a 'requestSync' which sets a flag causing capabilities to give up when they run out, does the same thing as the context switch timer to boot them out.  Parallel GC reuses the capabilities.  This is Haskell's "stop the world", similar things occur when the number of capabilities change

** Some amount of delicacy deciding when a GC is appropriate

MESSAGES
--------

Message INBOX - messages are used for MVars, for exceptions, thread migration, talk about it generally.

FFI AND THREADS
----------------

FFI (if you're not interested in how to support FFI, can skip this section) ~~ tied to load balancing
    Citation: Extending the Haskell Foreign Function Interface with Concurrency
        Note: this paper predates multithreaded Haskell execution (XXX what changed?)
        Update: "Another OS thread is waiting for capability because of foreign call-in", I think
            with concurrent Haskell that's just not necessary
    This multiplexing design is very simple, and the obvious thing to do.  But there are problems: consider blocking foreign calls (other problems described in the paper: thread-local state in FFI, multi-threaded clients, callbacks)--unification of mechanism! (so bound threads are generated by incalls)

    Idea: BOUND thread.  Intuitively, foreign functions invoked on bound thread run on associated OS thread.  TSOs now need to have a pointer to an InCall structure (bound)

    New operations to support:
         Haskell <-----------------> Foreign
      bound/unbound  (return)   native/recursive

    Modification to scheduler loop:
        maybeYieldCapability
            checks if GC is happening, or
                   cap->returning_tasks_hd != NULL (task is returning from foreign call),
                        ^--- done in a special way so that they have priority
                   or the bound case seen below...
        tso = popRunQueue
        if (tso->bound) {
            if tso bound to this task, OK
            otherwise, give the capability to the proper task (implemented by shouldYieldCapability and releaseCapability)
        } else {
            if this is an incall, yield the capability to another task (essentially, don't want to use a native thread that someone else may need to run random other Haskell code)
        }

        when the thread finishes

        in compiled code, safe foreign calls give up capability to another worker (suspendThread/resumeThread, putting the tso into an suspended ccalling threads; read this code to figure out what is going on here)

        Something about 'Passing around the Capability'

    Important invariant: if our capability is put on the wrong task, then the "correct" task is sleeping.  (Actually, one might hope to arrange things so that the capability is never put on the wrong task, but maybe if there are some unbound threads this is fine.)  This comes from another invariant: an OS thread can be bound to at most ONE Haskell thread (so if you create a new Haskell thread which needs to share the state, you need to arrange for the original thread to run your calls!)

    Unbound threads execute on worker threads.

    Load balancing: want to push a thread from one capability to another.  Ordinarily, you'd have to synchronize this (since they're running on different threads). Idea: only want to push work if there are IDLE capabilities (otherwise, it doesn't help much.) Now, since capabilities are decoupled from OS threads, an idle capability is NOT RUNNING and so you can just take the lock on it, stuff some threads on it, and then animate it.
        Simple extension: "locked" flag, which prevents threads from being pushed between capabilities

** Also talk about stable pointer design... (it's a giant hash table)

We should abstract some of these details.  Don't have to go into the details of the locks and condition variables is.

INTERPROCESS COMMUNICATION: MVARS
---------------------------------

Interprocess communication!

Stupid way to do it: add locks, mutexes and the other usual suspects

Idea: we manage the scheduling of processes ourselves.  So what if we have a system where one thread can hand of execution to another thread with out incurring a roundtrip through an OS context switch

XXX this will be pretty easy to explain, and I've looked at this closely

Send a message to wakeup a thread on another capability

INTERPROCESS COMMUNICATION: STM
-------------------------------

!!! This will be a difficult section to write !!!

*** non-trivial interaction with synchronous exceptions

Maybe this is not one of the really important topics

AYNCHRONOUS EXCEPTIONS
----------------------

Previous mechanisms describe how to do communication when processes are cooperating.  What about when they're not? (e.g. consider the interrupt mechanism of Unix.)


SPARKS AND THREADS
------------------

XXX need to know about thunks! (maybe defer this until we talk about lazy evaluation)

If the run queue is empty, what should we do? Run sparks! (by creating a spark thread--batching the evaluation together; scheduleActivateSpark).  Spark thread grabs sparks from its own pool or other pools and whnf's them, but aborts if the capability gets any real work.

spark pool per Capability

balancing (balanceSparkpoolsCaps) using bounded work-stealing queue.  Choice of FIFO (oldest--needs to be synchronized) or LIFO (youngest). Experimentally oldest is better

GC fizzling sparks: when thunk is already evaluated (use pointer tagging XXX forward reference!)

MUMBLE something about other sparks to retain (GC policy)--XXX need to check what it is doing now

**********

Concurrency for LAZY EVALUATION
-------------------------------

    run queues are now now per capability/same with remembered set
    thunks are a big problem
    usual: enter, push update frame, pop to update frame
    bad idea: locking thunks. so go lock free
    result word to make sure indirection rewrite will not clobber fvs; memory fence to ensure indirection is seen properly
    expensive duplicate work: deal with black holing, by periodically scanning the stack and CASing; truncate stack if someone else had blackholed it; need to mark update frames (lazy blackholing)
    OUT OF DATE PAPER: per TSO black hole queues, a little delicate
    freezing state of execution
    greyholing
    some funny STM stuff

