\section{Lazy evaluation}

%   lazy evaluation: what is it?
%   implemented using self-update model; this is flexible and reduces code size, but introduces inefficiency
%   haskell is lazy by default, so it needs to be efficient
%   some of these techniques applicable to other languages, or general memoizing code

\emph{Lazy evaluation} is an evaluation strategy which delays the
evaluation of an expression until its result is requested.
Operationally, an expression does not result in a computation, but
rather results in the allocation of a \emph{thunk}, which can be forced
in order to produce the true result of the expression.  This result
is then written over the thunk, so that future accesses to the thunk
avoid repeated evaluation.

There are a number of different strategies for implementing lazy
evaluation.  Their primary difference lies in who is responsible for
updating the thunk: is it the evaluator of the thunk, or the thunk
itself?  The former constitutes the \emph{cell model}, where the
call-sites of thunks check if the thunk is evaluated before calling in
and write back the result on return; the latter constitutes the
\emph{self-updating model}, where call-sites unconditionally jump to (or
\emph{enter}) the thunk, leaving the thunk responsible for updating
itself.  GHC utilizes a self-updating model, partially a historical
artifact from when \emph{all} objects on the heap (including data
constructors) were expected to be evaluated by entering them, and
because it works well with dynamic pointer tagging
(Section~\ref{sec:tagging}), which avoids performing the jump for
evaluated thunks at the cost of a cheap register comparison (no memory
access).

Most languages give very little thought to the efficient implementation
of thunks, since they are not a core part of the language.  However,
Haskell is lazy by default, and thus the speed of thunks is critical to
the overall performance of most Haskell programs.  As a result, GHC has
a very efficient implementation of thunks.  This section discusses three
important aspects of efficient implementation of thunks: how to
efficiently determine if a thunk is already evaluated
(Section~\ref{sec:tagging}), how to safely implement thunk update in a
multithreaded setting (Section~\ref{sec:sync}), and how to avoid
duplicated work evaluating a thunk when two threads attempt to evaluate
the same thunk at the same time (Section~\ref{sec:blackhole}).

%   Unusually, the info table also contains \emph{code} for the object: any
%   object can be ``entered'' (by jumping to the code in the infotable) in
%   order to evaluate it.  This is perhaps the most important design
%   decision in STG: it prescribes a uniform data representation for both
%   fully evaluated data values (constructors and functions) and unevaluated
%   thunks---referred to collectively as \emph{closures}.

%   In the original
%   design of the STG, all code wanting to access a field in a data
%   structure first entered the closure and upon return would receive the
%   values of the data structure (a \emph{vectored return}).  The lack of
%   any check whether or not a thunk was evaluated or not (an obvious
%   alternative implementation strategy) is behind the ``tagless`` in STG's
%   name.

%   \SM{We don't do vectored returns, since GHC 4.0. Also
%     return-in-registers, which you might also find menioned in the
%     original STG paper, was removed in favour of unboxed tuples.}

%   \SM{Tagless is a bit debatable these days.  We have tags in pointers
%     (pointer tagging).  Also the code pointer for a function closure is
%     the entry code for the function (not the ``eval'' code).}

\section{Dynamic pointer tagging} \label{sec:tagging}

In the na\"ive self-updating model, we always perform an indirect jump
to the entry code of a heap object before accessing its fields.  If the
object is already a data constructor, this jump returns immediately as a
no-op.  Unfortunately, these indirect jumps are poorly supported by
modern branch prediction units on processors.  Ideally, we would like
to avoid making a jump at all when it is unnecessary.

Modern GHC implements a \emph{dynamic pointer tagging}
scheme~\cite{Marlow2007} to provide information on whether or not a heap
object is evaluated or not.  This scheme works by using the lower order
bits (two bits in a 32-bit machine, and three bits in a 64-bit machine)
in order to record whether or not the contents of a pointer are already
evaluated.  If the lower order bits are zero, then the pointer is
unevaluated and we need to enter the closure.  Otherwise, these bits can
be used to record which constructor the object is, e.g. for booleans, a
tag of 1 would indicate \verb|False| and tag of 2 would indicate
\verb|True|.  Pointer tags are easily added to an object when it is
initially allocated (the tag never changes, because data on the heap is
immutable) and must be preserved by the garbage collector.

Pointer tagging is well worth considering even for non-lazy-by-default
languages.  Because case analysis on a tagged pointer can be done
without any memory accesses, tagged pointers enable user defined data
types to be nearly as efficient as built-in types (e.g. booleans), while
at the same time allowing for fields in the constructors to store extra
information.  And of course, they are essential for thunks!

\subsection{Synchronization} \label{sec:sync}

%   In the section on concurrency and parallelism, we asserted that
%   execution of Haskell threads could be made parallel, assuming that the
%   compiled Haskell code was thread-safe.  As any developer who has needed
%   to write thread-safe code can attest, this is a tall order!
%   Fortunately, much of this work is already done: both explicit interthread
%   communication and pure code can be synchronized with little fuss.

We now turn our attention to how thunks are updated.  Updating a thunk
with its new value constitutes mutation on shared state, thus, thunks
update is an important obstacle on the way to
thread-safety.\footnote{One might say it's the only obstacle, as pure
code requires no synchronization and explicit interthread communication
utilizes similarly explicit synchronization.}  A na\"ive approach is to
synchronize all of the updates.  This is extremely costly: Haskell
programs do a lot of thunk updates!

Once again, our saving grace is purity: as thunks represent pure
computation, evaluating a thunk twice has no observable effect: both
evaluations are guaranteed to come up with the same result.  Thus, we
should be able to keep updates to thunk unsynchronized, at the cost of
occasional duplication of work when two threads race to evaluate the
same thunk.

A race can still be harmful, however, if we need to update a thunk in
multiple steps and the intermediate states are not valid.  In the case
of a thunk update, we need to both update the header and write down a
pointer to the result; if we update the header first, then the
intermediate state is not well-formed (the result field is empty); on
the other hand, if we update the result field first, we might clobber
important information in the thunk.  Instead, we leave a blank word in
every thunk where the result can be written in non-destructively, after
which the header of the closure can be changed.\footnote{Appropriate
write barriers must be added to ensure the CPU does not reorder these
instructions.}

\begin{verbatim}
word    step 1   step 2   step 3
   0     THUNK    THUNK      IND \ valid closure
   1         -   result   result /
   2   payload  payload  payload <- payload is slop
\end{verbatim}

\subsection{Black holes} \label{sec:blackhole}

Some thunks take a long time to evaluate: we'd like to avoid duplicating
their work.  What we would like is for threads to notice when someone is
working on a thunk, and wait for the result to become available.

The mechanism by which this is implemented is a \emph{black hole}, which
represents a thunk that is currently being evaluated.  A thunk can be
\emph{claimed} by overwriting it with a black hole.  Black holes were
originally proposed as a solution for a space leak that occured in some
cases of tail calls~\cite{Jones2008}, but they have found their utility
in a multithreaded setting.  Recall that a thread wishing to evaluate
a thunk jumps to the entry code; the entry code of a black hole
places a thread on the blocked queue of the owner the black hole, to
be woken up when the thunk has been evaluated.\footnote{It is somewhat difficult
to put a blocked queue on the thunk itself (due to the lack of
synchronization); instead, GHC uses a per-thread list of black hole
blockers which is traversed every time a thread finishes updating a
thunk.}

There are two times when a black hole can be written: it can be \emph{eagerly}
written as soon as a thunk is evaluated, or it can be \emph{lazily} deferred
for when a thread has gotten descheduled (and thus the thunk was taking
a long time to evaluate.)  If a black hole is written eagerly,
it is on the fast path of thunk updates, and we cannot use
synchronization.  We call these \emph{eager black holes} (also known as
\emph{grey holes}), which do not guarantee exclusivity.  Lazy blackholing is done more infrequently, and thus
we can afford to use a CAS to implement them.\footnote{While multiple
    threads may have eagerly blackholed a thunk, we guarantee only one
    thread has lazily blackholed it.  If a thunk \emph{must not} be
duplicated, it can achieve this by forcing all of its callers to perform
lazy blackholing
(\texttt{noDuplicate\#}).  \texttt{unsafePerformIO} uses precisely
this mechanism in order to avoid duplication of IO effects.}

The upshot is that GHC is able to implement lazy evaluation without any
synchronization for most thunk updates, applying synchronization
\emph{only} when it is specifically necessary. The cost of this scheme
is low: a single extra field in thunk and a (rare) duplication of work
upon a race.
