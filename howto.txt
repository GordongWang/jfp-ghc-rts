HOW TO BUILD A GHC RUNTIME SYSTEM

You're building a runtime for Haskell. Where are you going to start?!

More questions
-------------

- Did we switch to "copy first then cas" for evacuate?

Some big questions
------------------

- How much technical detail should specific sections go into?
- Should sections be divided in a way that some "details" are not
  presented until later (versus considering all of the problems at once)
- Should exceptions be introduced before or after the execution model of
  Haskell is discussed?

- Giving the high level motivation, that's an important thing to get into,
  enough for them to understand why we did it this way.  But in order to replicate it they will probably
  need to read the full technical details.  But if it's important to understand.
- No need to describe in full detail what a block descriptor is.

- Making a fast curry - good paper on evaluation.  So certainly we don't
  want to go into the full details on the eval-apply.  But you need enough details.
  Take a stack of a thread and turn it into the object of the heap, in such a way that
  if someone demands the value later, you can reuse the computation.  So you need to explain enough about thunks,
  layout of the stack, update frames.

- Short intro to evaluation before threading; or maybe even before
  GC--but don't talk about lazy evaluation too long!  There are some things you just can't
  do without lazy evaluation, make some sort of argument why that would be useful.

- Important to talk about what the implementation of lazy evaluation;
  all of the stuff, e.g. async is we can only do it because we have lazy evaluation and we can suspend it on the heap.

GARBAGE COLLECTOR
-----------------

Well, Haskell is a managed memory language.  So you decide that you need to build a garbage collector.

Your first idea is to do generational garbage collection, because that's what everyone does, most garbage dies young.  This is true in most languages, but this is especially true in functional programming languages, where programs allocate a lot of memory. They allocate so much, that objects should not be promoted eagerly; they should be aged.

Where is the memory coming from?  Stupid thing is to just allocate a big honking block, but that's annoying.  Idea: heap is not a contiguous piece of memory, but a bunch of chained together blocks.  (Knock on effect: large objects can be pinned to blocks BF_LARGE. But watch out for fragmentation.)  (some details about how it works, esp. megablocks and interaction with OS memory allocator, and dealing with the free lists)
    Citation: DEB94 "Don't stop the BIBOP"
              Ste77 "Data representations in PDP-10 MacLISP"

How do we identify pointers?  Idea: require every heap object to be boxed, and store an info table saying what the pointers are.  Note: what is all of the # nonsense about? Well, if a value is unboxed, operations on it are really fast, but it can never hit the heap.
    So we need to talk a little bit about closure representation. At this point, if we just have some nondescript closure this is reasonable
    Pointers-first versus bitmap
    Need to talk about the stack, and temporary values in memory!
        JVM, CLR use a different technique, where they define the stack layout at *EVERY POINT IN THE BYTECODE*

OK, but generational garbage collection has the classic problem where old generations may point into young generation? But Haskell doesn't mutate very much, so we can do something stupid to deal with mutation: remembered sets.  (point out some of the stupid things we've done with arrays, etc in the past; need to manually add write barriers to those ops)

But that's not true: Haskell laziness means that we have to update a value when it gets evaluated.  Easy: eager promotion: if old generation ends up pointing to young generation (due to a write-once), promote the new object to the old generation.  Notice: need to scavenge old generations first!  (Thunks turn up as mutable objects)

OK, but we want to make it even faster.  Idea: parallel garbage collection.  How is work split up?  It is split up using the blocks.  (More details: partly free list).  Note: all of the old GC state now has to be made thread-local (some of it wasn't, for a while)  This is a good idea, and other languages should use this technique too! (Or maybe there's a reason why it doesn't work so well in other cases?)
    What about claiming objects for evacuation? (You can only do it once, see evacuation below)
    Termination criterion FDSZ01 (check for zero running threads)
    Citation FDSZ01
             ABCS01

What are the roots?  Intuitively, whatever is "executing" on the heap is the roots. But we haven't talked about what executes in Haskell yet: more soon!

Note: empirically, evacuate() is the most important function for the performance of GC. "Now you know!"

GC summary:
    - Block(start, free, link, gc metadata)
    - Nursery is a bunch of blocks
    - A generation is a bunch of blocks + large objects (synchronized) + threads(?);
      linked to a "to generation"
    - gcthread unit of parallel collection, containing thread-local data;
      per generation (the workspace) has todo blocks and a scavenged list, also the
      part list optimization ** what's the overflow thing for?
    - Collection: collects all gens up to some point; does an evacuate
      and scavenge
**  Cheney's algorithm
    - Evacuation: figure out what block it's going into, and then copy
      the object there; note, can only do this ONCE, so if it needs to happen
      twice gotta put it in remembered set
    - Scavenge: looks at all the pointers and evacuates their contents
      (going from old generation to new)
** what is a mark stack?
    - Detail: thunk selector evaluation, to prevent GC from holding onto
      memory too long (defer?) ~ really interesting problem, but it would require a lot of details. I think that is going to be too much details (marlow); there are other approaches to the problem as well.  There's a language level solution which people say we should use in GHC; add a magic call construct

GarbageCollect(collect_gen, cap)
    take out storage manager lock
    initialize gct (gc thread data) for this thread
    start_gc_threads --> set gc_running_threads = 0 ### I think the comment here is misleading
    don't parallelize unless it's a major GC
    initialize generations/steps which are being collected
        throw away mutable list (not necessary, you see?)
        deprecate existing blocks in generation (i.e. put them in from-space)
        grab partial blocks from gc_thread workspace and put them in from-space
    initialize generations/steps which are not being collected
        stash the mutable list
    initialize gc threads
    gc_running_threads++
    wakeup threads
    scavenge stuff (no moving): capability mutable lists
    mark stuff (i.e. evacuate it): capabilities / CAFs / scheduler / weak pointer list / stable tables
    in a loop: scavenge until all done ***
    shut down gc threads
    collected generations: free from-space and make to-space from-space (same with large objects)
    uncollected generations: append promoted large objects
    reset nursery
    run finalizers
    resurrect threads
    give up excess memory back to operating system

^-- simplified
    throw out mutable lists for all generations being collected and move their blocks to from-space
    wakeup gc threads (setting gc_running_threads > 0)
    evacuate the roots (if there are multiple capabilities, assign those to the corresponding GC thread; otherwise only one GC thread gets everything to start off)
    scavenge until done
    formalities: run finalizers, resurrect threads, return memory to OS

*** don't talk about sharing partial blocks

scavenge_until_all_done()
    scavenge_loop(): repeatedly call scavenge_find_work()
        try to do local work (todo block)
        try to steal work (from todo_q)
    collect completed blocks and add them to gen (taking out spinlock)
    spin looking either for work or for completion (num threads = 0)
        looking for work checks the work stealing queue (also todo_overflow; it's the "one buffer" before things hit the queue)

scavenge # Cheney's algorithm invariant says that there is no SLOP so we can just walk down it
    loop, case split on pointer type
        evacuate the pointer fields, temporarily switching off eager promotion if object is mutable
        if evacuation fails for mutable object, mark as dirty and force it onto the mutable list
    push the block into "finished" if it's not todo (i.e. the to-space), checking if it's still got stuff left

evacuate
    check if is heap allocated
    check if it was already evacuated
    check if it's an indirection (either it is evacuated enough, or this counts as a fail)
    case split on type, and do a copy
        use a cas when replacing with forwarding pointer on certain objects, but not all
        e.g. thunks need locking, constructors don't
        If you get clobbered, just re-evacuate (your copy is dead, but it will get GC'd next round)

(partly free nonsense)

Citation "Parallel generational-copying garbage collection with a block-structured heap"
Citation Cheney "A nonrecursive list compacting algorithm"

** defer pinned objects
** defer CAFs
** defer static objects (HEAP_ALLOCED)

THREADS
-------

We've talked about how to allocate data. What does the runtime need to
support operating on this data?  At the very least, need to know about
the execution context to do GC properly (with concurrency, need to know more.)

What is an execution context?  Primarily, it is a STACK.  We need a way to
let the GC know what values on the stack are pointers and what are not.  So
reuse the info table layout!  (*** some nonsense about SRTs)  Stack frames/activation
records are also objects.
    Detail: "stack stubbing" mark live variables dead by replacing new stack frame
    requires bitmap layout

Sometimes, we will need to temporarily suspend execution to run a
garbage collection.  Constraint on code generation: there need to be
safe points to run the GC (otherwise you might accidentally be in the
middle of constructing a stack frame when you serviced an interrupt to
GC).  Easy thing to do: check on HEAP ALLOCATION.

OK, this works OK, but what about concurrent Haskell?  Concurrent
Haskell has a few parameters:
    1. Processes
    2. Atomically mutable state, i.e. mechanisms for communicating between processes

Naive idea: give each process its own OS thread (one-to-one).  Better idea: multiple
lightweight threads mapped onto a single operating system thread (multiplexed).  This is
cheap because we don't have to interact with expensive OS thread facilities.

Let us presuppose that we want multiplexing.  We now have a gap: "OS threads" ~~~~ "Haskell threads", this has consequences.

First, decouple Haskell thread representation from OS. Stacks are no longer OS stacks, thread now needs to store more information (e.g. the "thread-local" errno number, what the thread ID is, what the thread's run status is, what capability the thread is running on--this bit is important for IPC).  So turn this into first-class objects and store them on the heap.  (Implementation detail: one monolithic stack or stack chunks? GHC's changed its mind over the years, now we use stack chunks).  Performance note: stacks have dirty bits, so we don't have to repeatedly scan them when threads haven't run.

Second, we need to map Haskell threads onto operating system threads, so they actually can run.

Idea: take your operating system threads (call it a Haskell Execution Context or Capability), and give them a run queue of Haskell threads (a simple doubly linked list--this means TSOs are MUTABLE with POINTERS to STATE--recall GC injunctions).  The loop goes:

    while(1) {
        tso = popRunQueue(cap)
        result = StgRun(tso)
        case result of
            out of heap -> re-enqueue tso; call GC
                Note about large blocks: need to add the large block to the nursery which is per CAPABILITY, try to run the thread immediately afterwards
            out of stack -> enlarge stack; re-enqueue tso
            time expired -> put tso on end of queue
                Note about how the context switch timer works: it sets a variable (Hp) which then causes Haskell to boot out of the loop and back to the scheduler
            finished -> continue
    }

Notice: Garbage collecting processes: the RUN QUEUE is the root!  (If the system is deadlocked, a GC can identify groups of deadlocked threads... but see later about resurrecting threads with exceptions--this requires us to talk about weak pointers)

Note: garbage collection requires ALL capabilities to be acquired, since we do NOT have a concurrent collector.  Do a 'requestSync' which sets a flag causing capabilities to give up when they run out, does the same thing as the context switch timer to boot them out.  Parallel GC reuses the capabilities.  This is Haskell's "stop the world", similar things occur when the number of capabilities change

** Some amount of delicacy deciding when a GC is appropriate

MESSAGES
--------

Message INBOX - messages are used for MVars, for exceptions, thread migration, talk about it generally.

ABI NOTES ABOUT STACKS
----------------------

C stack is used for temporary data
Haskell stack is NOT used for temporary data

Every entry on the Haskell stack has an info table, just like heap objects.  These info tables point to code, which is how "return works". (As it turns out, heap objects also get code, but that has to do with laziness!)  So the stack is unsuitable for storing data temporarily, e.g. when you've run out of registers.

    - put stack onto the heap (reifying control for resumability)
    - stack frames do not necessarily correspond directly to structure
      of code.  Lots of Haskell code will push multiple stack frames.  So they are a bunch of continuations!

Since data on the C stack could point to the heap, when we hit a GC all of that data needs to be pushed into an info table.  So clearly a GC cannot happen "just anywhere."  Thus, GC's can only occur at the *starts* of functions (e.g. when we are doing the heap check)

Returning on the stack is single return

Info tables are a nice intensional structure, so there are some "wired-in" info tables which the runtime system handles manually. Examples:

    - stg_upd_frame (and variants)
    - stg_stack_underflow_frame (UNDERFLOW_FRAME) ~~ also notice with stack chunks have to
      copy data and deal with running out of info on a chunk; also stg_stop_thread (STOP_FRAME)
    - CATCH_FRAME
    - ATOMICALLY_FRAME
    - CATCH_STM_FRAME
    - CATCH_RETRY_FRAME

Like most other stack based languages, we walk up the stack when making non-local jumps of control

FFI AND THREADS
----------------

FFI (if you're not interested in how to support FFI, can skip this section) ~~ tied to load balancing
    Citation: Extending the Haskell Foreign Function Interface with Concurrency
        Note: this paper predates multithreaded Haskell execution (XXX what changed?)
        Update: "Another OS thread is waiting for capability because of foreign call-in", I think
            with concurrent Haskell that's just not necessary
    This multiplexing design is very simple, and the obvious thing to do.  But there are problems: consider blocking foreign calls (other problems described in the paper: thread-local state in FFI, multi-threaded clients, callbacks)--unification of mechanism! (so bound threads are generated by incalls)

    Idea: BOUND thread.  Intuitively, foreign functions invoked on bound thread run on associated OS thread.  TSOs now need to have a pointer to an InCall structure (bound)

    New operations to support:
         Haskell <-----------------> Foreign
      bound/unbound  (return)   native/recursive

    Modification to scheduler loop:
        maybeYieldCapability
            checks if GC is happening, or
                   cap->returning_tasks_hd != NULL (task is returning from foreign call),
                        ^--- done in a special way so that they have priority
                   or the bound case seen below...
        tso = popRunQueue
        if (tso->bound) {
            if tso bound to this task, OK
            otherwise, give the capability to the proper task (implemented by shouldYieldCapability and releaseCapability)
        } else {
            if this is an incall, yield the capability to another task (essentially, don't want to use a native thread that someone else may need to run random other Haskell code)
        }

        when the thread finishes

        in compiled code, safe foreign calls give up capability to another worker (suspendThread/resumeThread, putting the tso into an suspended ccalling threads; read this code to figure out what is going on here)

        Something about 'Passing around the Capability'

    Important invariant: if our capability is put on the wrong task, then the "correct" task is sleeping.  (Actually, one might hope to arrange things so that the capability is never put on the wrong task, but maybe if there are some unbound threads this is fine.)  This comes from another invariant: an OS thread can be bound to at most ONE Haskell thread (so if you create a new Haskell thread which needs to share the state, you need to arrange for the original thread to run your calls!)

    Unbound threads execute on worker threads.

    Load balancing: want to push a thread from one capability to another.  Ordinarily, you'd have to synchronize this (since they're running on different threads). Idea: only want to push work if there are IDLE capabilities (otherwise, it doesn't help much.) Now, since capabilities are decoupled from OS threads, an idle capability is NOT RUNNING and so you can just take the lock on it, stuff some threads on it, and then animate it.
        Simple extension: "locked" flag, which prevents threads from being pushed between capabilities

** Also talk about stable pointer design... (it's a giant hash table)

We should abstract some of these details.  Don't have to go into the details of the locks and condition variables is.

INTERPROCESS COMMUNICATION: MVARS
---------------------------------

Interprocess communication!

Stupid way to do it: add locks, mutexes and the other usual suspects

Idea: we manage the scheduling of processes ourselves.  So what if we have a system where one thread can hand of execution to another thread with out incurring a roundtrip through an OS context switch

XXX this will be pretty easy to explain, and I've looked at this closely

Send a message to wakeup a thread on another capability

INTERPROCESS COMMUNICATION: STM
-------------------------------

!!! This will be a difficult section to write !!!

*** non-trivial interaction with synchronous exceptions

Maybe this is not one of the really important topics

EVALUATION MODEL
----------------

SYNCHRONOUS EXCEPTIONS
----------------------

AYNCHRONOUS EXCEPTIONS
----------------------

Previous mechanisms describe how to do communication when processes are cooperating.  What about when they're not? (e.g. consider the interrupt mechanism of Unix.)

We've got to talk about NORMAL EXCEPTIONS, and this ties into the EXECUTION MODEL

SPARKS AND THREADS
------------------

XXX need to know about thunks! (maybe defer this until we talk about lazy evaluation)

If the run queue is empty, what should we do? Run sparks! (by creating a spark thread--batching the evaluation together; scheduleActivateSpark).  Spark thread grabs sparks from its own pool or other pools and whnf's them, but aborts if the capability gets any real work.

spark pool per Capability

balancing (balanceSparkpoolsCaps) using bounded work-stealing queue.  Choice of FIFO (oldest--needs to be synchronized) or LIFO (youngest). Experimentally oldest is better

GC fizzling sparks: when thunk is already evaluated (use pointer tagging XXX forward reference!)

MUMBLE something about other sparks to retain (GC policy)--XXX need to check what it is doing now

**********

LAZY EVALUATION
---------------------

STG paper
    Explore design space: How are function values, data values and unevaluated expressions represented? how is function application performed? How is case analysis performed on data structures?
    hnfs (functions and constructors) and thunks: all called 'closures'
        Cheap indirections available (eliminated by GC)
        Black holes, concurrency handled exceptionally
        Can avoid heap allocation
    data
        function - code + free variables
        thunks - could be parameterless function value, but should physically update with new value.  Cell model has each closure with status flags; self-updating has code for a thunk which handles the update. if value takes up more space need to replace with indirection.  black holes to check infinite loops and wasted work.
        PAPs (partial application)
    OLD functions - old scheme was push/enter, breaking activation frames
        eval/apply: caller knows what the arguments are, looks at function closure, finds arity, makes exact call
            lots and lots of call continuations
    OLD data types - uniform across "built ins" and user-defined, vectored returns
unboxed values
operational intuition: code/stack/heap

header/payload; executable code, type, layout info, type info
stack frame thougth as a stack-allocated function closure

Spineless Tagless G-Machine, except with eval/apply and dynamic pointer tagging

tagging - get the branch predictor less terrible
    claims that it's OK to have tag pointer be zero, but THIS IS NOT TRUE http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution/PointerTagging
    initialising/using/clearing/propagating
    this IS implemented for functions (showing arity)
    note: NOT the GC tag bigs

Concurrency:
    run queues are now now per capability/same with remembered set
    thunks are a big problem
    usual: enter, push update frame, pop to update frame
    bad idea: locking thunks. so go lock free
    result word to make sure indirection rewrite will not clobber fvs; memory fence to ensure indirection is seen properly
    expensive duplicate work: deal with black holing, by periodically scanning the stack and CASing; truncate stack if someone else had blackholed it; need to mark update frames (lazy blackholing)
    OUT OF DATE PAPER: per TSO black hole queues, a little delicate
    freezing state of execution
    greyholing
    some funny STM stuff

----

Operationally, in the STG-machine implementation in GHC, the
case expression is compiled to code that enters the variable being
scrutinised, after pushing on the stack a return address, or contin-
uation, for the alternatives. All heap objects have the uniform rep-
resentation shown in Figure 1. The first word of every object is an
info pointer, which points both to the entry code for the object, and
to the preceding info table for the object. The info table describes

Data Write Misses
    Sequential heap writes: there is no need to read the memory block into D1 cache upon a write miss, because it will soon be overwritten.  You can do this with a write-allocate cache with sub-block placement, but this is not frequent; or could use write-invalidate instruction (x86 does not have this, but ARM does!)
    Alternative is prefetch: fetch ahead 64 bytes in GC; this should help 22%
Data read miss
    Code next to data pollutes data cache with instructions (TABLES_NEXT_TO_CODE=off; but now there's another indirection!)
    info tables are frequently accessed; maybe layout can be improved
    Idea: apply BIBOP to make a heap where all objects are one type of object
        >>> USE THIS TRICK FOR PROFILING!!!!
