\section{Concurrency and parallelism}

We now turn our attention to the implementation of
concurrency~\cite{PeytonJones:1996:CH:237721.237794} and
parallelism~\cite{Harris:2005:HSM:1088348.1088354} in the GHC runtime.
It is well worth noting the difference between concurrency and
parallelism: a parallel program uses multiple processing cores in order
to speed up computation, while a concurrent program simply involves
multiple threads of control, which notionally execute ``at the same
time'' but may be implemented merely by interleaving their execution.

GHC is both concurrent and parallel, but many of the features we
describe are applicable in non-parallel but concurrent systems (e.g.
systems which employ cooperative threading on one core): indeed, some
were developed before GHC had a shared memory multi-processor
implementation.  Thus, in the first section, we consider how to
implement \emph{threads} without relying on hardware support.  We then
describe a number of inter-thread communication mechanisms which are
useful in concurrent programs (and say how to synchronize them). Finally
describe what needs to be done to make \emph{compiled} code thread-safe.

\subsection{Threads}

Concurrent Haskell~\cite{PeytonJones:1996:CH:237721.237794} exposes the
abstraction of a \emph{Haskell thread} to a programmer. As the
operating system also provides native threads, one may wonder if there
any difference between a Haskell thread and an OS thread.

Many languages with multithreading support simply expose OS threads: a
``thread'' is one and the same as an OS thread.  While simple, this
approach has costs.  In particular, users of these languages must be
economical in their use of threads, as most operating systems cannot
support thousands of simultaneous OS threads.  This is a shame, because
in many applications the most natural way to \emph{structure} a program
involves thousands of logical threads: consider a web server, for
example, which logically has a thread of execution per request.
Furthermore, when a program cannot be made thread-safe and must be run
in a single OS thread, no concurrency is available; it must be
implemented in userspace, as is the case with many asynchronous IO
libraries.

To support cheap threads, we must \emph{multiplex} multiple Haskell
threads onto a single OS thread.  A Haskell thread runs until it is
\emph{preempted}, at which point we suspend its execution and switch to
running another thread, an operation handled by the \emph{scheduler}.
How is this preemption implemented?  True preemption is difficult to
implement, because it implies we can interrupt the execution of code any
any point, even if it would be leaving some data structures in an
intermediate state.  So instead, compiled Haskell code yields
cooperatively\footnote{Cooperative in the sense that the compiled code
has explicit yield points, not in the sense that the code a programmer
has to write contains yield points.} at various safe points, where we
know that the heap is in order and our internal state is saved (to be
restored on resumption).  These points are in fact when Haskell code
performs a \emph{heap check} to find out if there is enough free memory
on the heap to perform an allocation.  These checks automatically are
safe, because Haskell code already needs to be able to yield to the
garbage collector, in case we have run out of memory.  This check is
extended to also yield when a thread has been preempted.\footnote{ In
practice, preemption is handled by way of a timer signal, which, when
fired, sets the heap limit to zero, triggering a faux ``heap overflow''
which the scheduler can then identify as a preempt and not a true
request for garbage collection.  Thus, the heap check and preemption
check is a single conditional branch.}

Once we have a way of suspending and resuming threads, the scheduler
loop is quite simple. Maintain a \emph{run queue} of threads, and repeatedly:

\begin{enumerate}
    \item Pop the next thread off of the run queue,
    \item Run the thread until it yields,
    \item Check why the thread exited:
        \begin{itemize}
            \item If it ran out of heap, call the GC and then run the
                thread again;
            \item If it ran out of stack, enlarge the stack and then run
                the thread again;
            \item If it was preempted, push the thread to the end of the queue;
            \item If the thread exited, continue.
        \end{itemize}
\end{enumerate}

\subsection{Foreign function interface}

The benefit of lightweight concurrency is that it offers a way of
producing threads of control which are indistinguishable from OS
threads, but are dramatically cheaper.  However, there are some places
when this illusion does not hold: of particular note is the
\emph{foreign function interface} (FFI)~\cite{Marlow04extendingthe},
which permits Haskell code to call out to foreign code not compiled by
GHC, and vice versa.  What happens if an FFI call blocks?  As our
concurrency is cooperative, if an FFI call refuses to return to the
scheduler, the execution of all other threads will grind to a halt.
There isn't really any way around this problem without introducing true
OS threads to the mix: what we'd like to do is arrange for the blocking
FFI call and the scheduler to run on different OS threads concurrently.

As it turns out, it is simpler to move the scheduler to a different OS
thread than the FFI. Thus, we decompose OS threads into two parts: the
OS thread itself (called a \emph{task} in GHC terminology), and the
Haskell execution context (called a \emph{capability}).  The Haskell
execution context contains the scheduler loop and is responsible for the
contents of the run queue: when executing, it is owned exclusively by
the particular task (OS thread) which is running it.  A single-threaded
Haskell program has one capability: the capability is a \emph{global
lock} on the Haskell runtime.  Now, before a blocking FFI call is made,
the task releases the capability: if there is another idle \emph{worker
thread}, it can acquire the now free capability and continue running
Haskell code.

When an FFI call returns, we'd like to return the capability to the
original OS thread. Thus, we have to modify the scheduler loop as follows:

\begin{enumerate}
    \item Check if we need to yield the capability to some other OS thread, e.g. if an FFI call has just finished,
    \item \emph{Run as before.}
\end{enumerate}

Another place where Haskell threads differ from OS threads is thread
local state.  As capabilities are passed around OS threads, we make no
guarantee that any given FFI call will be performed on the same OS
thread as the previous FFI call.  To accomodate Haskell threads which
rely on thread-local state, Haskell introduces a \emph{bound thread},
which binds a Haskell thread to a fresh OS thread.\footnote{GHC
    also calls these \emph{in-calls}, due to the fact that external code
    which calls into Haskell must be bound: if it makes the Haskell code
calls out via the FFI again, the inner and outer C code may rely on the
same thread local state.}

How can we support bound threads?  A simple scheme is to give each bound
thread its own OS thread.  However, if we have only one capability, we
need to coordinate these new OS threads so that only one is running
Haskell code at a time.  We can do this by, once again, passing the
capability to whichever OS thread truly needs to run the bound thread:

\begin{enumerate}
    \item \emph{Check if we need to yield the capability to some other OS thread, e.g. if an FFI call has just finished,}
    \item \emph{Pop the next thread off of the run queue,}
    \item Check if the thread is bound:
        \begin{itemize}
            \item If the thread is bound but is already scheduled on the OS thread, proceed.
            \item If the thread is bound but on the wrong OS thread, give the capability to the correct task.
            \item If the thread is not bound but this OS thread is bound, give up the capability, so that any capability that truly needs this OS thread will be able to get it.
        \end{itemize}
    \item \emph{Run as before.}
\end{enumerate}

While the movement of capabilities from task to task is somewhat intricate,
it imposes no overhead when bound threads are not used.

\subsection{Load balancing}

Assuming that the compiled Haskell code is thread safe~(see
Section~\ref{sec:sync}), it is now very simple to parallelize execution:
allocate multiple capabilities!  Each OS thread in possession of a
capability runs the scheduler loop, and everything works the way you'd
expect.

There is one primary design choice: should each capability have its own
run queue, or should there be a single global run queue?  A global run
queue avoids the need for any sort of load balancing, but requires
synchronization and makes it difficult to keep Haskell threads running on
the same core, destroying data locality.  With separate run queues, however,
threads must be load balanced: one capability could accumulate too many
threads while the other capabilities idle.

The very simple load balancing scheme GHC uses is as follows: when a
capability runs out of threads to run, it suspends itself (releasing its
lock) and waits on a condition variable.  When a capability has too many
threads to run (it checks each iteration of its schedule loop), it takes
out locks on as many idle capabilities as it can and pushes its excess
threads onto their run queues.  Finally, it releases the locks and
signals on each idle capabilities that they can start running.  The
benefit of this scheme is that the run queues are kept completely
unsynchronized, but a plausible alternative is to use work-stealing
queues.

\subsection{Sparks}

\emph{Sparks}~\cite{Marlow2009} are a mechanism for speculative parallel computation.
When a program is not utilizing all of its CPUs, the other CPUs can be
reallocated to evaluate thunks that the programmer indicated are likely
to be needed in the future.  These units of work are called sparks, and
they offer a mechanism for cheap, deterministic parallelism---in contrast to
Haskell threads, which are more expensive and nondeterministic.

Sparks take advantage of Haskell's lazy evaluation (Section~\ref{sec:lazy}) to provide a source
of units of work for sparking.  The ability to speculatively evaluate
thunks---a spark is not guaranteed to be evaluated---comes from the fact
that thunks encapsulate pure code and have no side effects.
Furthermore, as thunk update is thread safe (Section~\ref{sec:sync}),
they provide a natural mechanism of communicating the result of a
computation back to the thread that requested it.

Given thread-safe thunks, the implementation of sparks is quite simple:
when a computation is sparked, it is stored in a \emph{spark pool}
associated with a capability.  When a capability has no work to do (e.g.
its run queue is empty), it creates a \emph{spark thread}, which
repeatedly attempts to retrieve a spark from the capability's spark pool
and evaluate it.  Thunk update ensures the results get propagated back
to the main thread. If the capability receives any real work, it
immediately terminates the spark thread.

Whereas threads rarely need to be load balanced, sparks frequently need
to be migrated, as the capability that is generating sparks is likely to
be the one that is also doing real work.  Sparks are balanced using
bounded work-stealing queues~\cite{Arora:1998:TSM:277651.277678,Hendler2005}, where a spark thread goes and steals
sparks from other threads when it has none to execute.

One important optimization for sparks is removing sparks which will not
contribute any useful parallelism.  For example, if the spark is
evaluated in the main thread before a spark thread gets around to it,
the spark is \emph{fizzled} and should be removed from the spark pool.
Additionally, if a spark's thunk has no references to it (i.e. is dead),
then the result cannot possibly have an impact on program execution and
it should also be pruned.  It is relatively easy to check for both of
these conditions in the garbage collector, by traversing the spark pool
and checking if the spark points to a thunk that was successfully evacuated.\footnote{An alternate design is to have sparks be GC roots, so that an outstanding spark keeps its data alive. While this is convenient for the implementation of \emph{parallel strategies}, it can result in space leaks, and GHC no longer uses this strategy.}

\subsection{MVars}

Haskell offers a variety of ways for Haskell threads to interact with
each other.  We now describe how to implement MVars, the simplest form
of synchronized communication available to Haskell threads.  An MVar is
a mutable location that may be empty.  There are two operations which
operate on an MVar: \verb|takeMVar|, which blocks until the location is
non-empty, then reads and returns the value, leaving the location empty,
and \verb|putMVar|, which dually blocks until the location is empty,
then writes its value into location.  An MVar can be thought of as a
lock when its contents are ignored.

The blocking behavior is the most interesting aspect of MVars:
ordinarily, one would have to implement this functionality using a
condition variable.  However, because our Haskell threads are not
operating system threads, we can do something much more lightweight:
when a thread realizes it needs to block, it simply adds itself to a
\emph{blocked threads queue} corresponding to the MVar.  When another
thread fills in the MVar, it can check if there is a thread on the
blocked list and wake it up immediately.

This scheme has a number of good properties.  First, it allows us
to implement efficient \emph{single wake-up} on MVars, where only one of
the blocking threads is permitted to proceed. Second, using a FIFO
queue, we can offer a fairness guarantee, which is that no thread
remains blocked on an MVar indefinitely unless another thread holds the
MVar indefinitely.  Finally, because threads are garbage collected
objects, if the MVar a thread is blocking on becomes unreachable,
\emph{so does the thread.}  Thus, in some cases, we can tell when
a blocked thread is deadlocked and terminate it.

\subsection{Asynchronous exceptions}

MVars are a cooperative form of communication, where a thread must
explicitly opt-in to receive messages.  Asynchronous
exceptions~\cite{Marlow:2001:AEH:378795.378858}, on the other hand,
permit threads to induce an exception in another thread without its
cooperation.  Asynchronous exceptions are much more difficult to program
with than their synchronous brethren: as a signal can occur at any point
in a program's execution, the program must be careful to register
handlers which ensure that any resources are released and invariants are
preserved.  In \emph{pure} functional programs, this requirement is
easier to fulfill, as pure code can always be safely aborted.
Asynchronous exceptions are quite useful in a variety of situations,
including timing out long running computation, aborting speculative
computation and handling user interrupts.

Triggering an asynchronous exception is relatively simple with
preemptive scheduling: force the target thread back to the scheduler,
at which point the scheduler can introduce the exception and walk up the
stack, looking for exception handlers.  In
case a thread is operating in a sensitive region, an exception masking
flag can be set, which defers the delivery of the exception (it is saved
to a list of waiting exceptions on the thread itself).

There are two primary differences between how asynchronous exceptions
and normal exceptions are handled.  The first is that a thread which is
messaged may have been blocking on some other thread (i.e. on an MVar);
thus, when an asynchronous exception is received, the thread must remove
itself from the blocked list of threads.\footnote{If your queues are
    singly linked, you will need some cleverness to entries. GHC does
    this by stubbing out an entry with an indirection, the very same
that is used when a thunk is replaced with its true value, and modifying
queue handling code to skip over indirections; because blocking queues
live on the heap, the garbage collector will clean it up for us in the end.}

\Red{This explanation probably still needs a little work}

The second difference is how lazy evaluation is handled. When pure code
raises an exception, referential transparency demands that any other
execution of that code will result in the same exception.  Thus, while
we are walking up the stack, when we see an \emph{update frame}, which
is a continuation responsible for taking a value and saving it to the
thunk (overwriting it), we go ahead and instead overwrite the thunk with a new
thunk that always throws the exception.  However, in the case of an
asynchronous exception, the code could have simply been unlucky: when
someone else asks for the same computation, we should simply resume
where we left off.  Thus, we instead \emph{freeze} the state of
evaluation by saving the current stack into the thunk.~\cite{Reid1999}
This involves walking up the stack and performing the following
operations when we encounter an update frame:

\begin{enumerate}
    \item Allocate a new closure (called an \verb|AP_STACK| closure, for
        ``apply stack'') which contains the contents of the stack above
        the frame, and overwriting the old thunk\footnote{Possibly a
        black hole.} with a pointer to this closure,
    \item Truncate the stack up to and including the update frame, and
    \item Push a pointer to the new \verb|AP_STACK| closure onto the stack.
\end{enumerate}

The result is a chain of \verb|AP_STACK| closures, where the top of each
frozen stack links to the next frozen stack.  When another thread
evaluates an \verb|AP_STACK| closure (intending to evaluate the thunk),
it pushes the frozen stack onto the current stack, thus resuming the
computation.

\subsection{STM}

Software Transactional Memory, or STM, is an abstraction for concurrent
communication which emphasizes transactions as the basic unit of
communication.  The big benefit of STM over MVars is that they are
composable: while programs involving multiple MVars must be very careful
to avoid deadlock, programs using STM can be composed together
effortlessly.

Before discussing what is needed to support STM in the runtime system,
it is worth mentioning what we do not have to do.  In many languages,
an STM implementation must also manage all side-effects that any code
in a transaction may perform.  In an impure language, there may be many
of these side-effects (even if they are local), and the runtime must
make them transactional at great cost.  In Haskell, however, the type system
enforces that code running in an STM transaction will only ever perform
pure computation or explicit manipulation of shared state.  This eliminates
a large inefficiency that plagues many other STM implementation.

\Red{How is it implemented}

\Red{Maybe move this below messages and white holes}

\subsection{Messages and white holes}

In the descriptions above, we said very little about the synchronization
that is necessary to implement them in a multiprocessor environment.
Under the hood, the GHC runtime has two primary methods of synchronization:
\emph{messages} and \emph{white holes} (effectively a spinlock).  The
runtime makes very sparing use of OS level condition variables and
mutexes, since they tend to be expensive.

GHC uses a very simple message passing architecture to pass messages
between capabilities.  A capability sends a message by:

\begin{enumerate}
    \item Allocating a message object on the heap;
    \item Taking out a lock on the message inbox of the destination capability;
    \item Appending the message onto the inbox;
    \item Interrupting the capability, using the same mechanism as the context switch timer (setting the heap limit to zero); and
    \item Releasing the lock.
\end{enumerate}

This allows the message to be handled by the destination capability at
its convenience, i.e. after the running Haskell code yields and we
return to the scheduling loop.  In general, the benefit of message
passing systems is that they remove the need for synchronizing any of
the non-local state that another capability \emph{might} want to modify: instead,
the capability just sends a message asking for the state to be modified.

When sending a message is not appropriate, e.g. in the case of
synchronized access to closures which are not owned by any capability in
particular, GHC instead uses a very simple spinlock on the closure
\emph{header}, replacing the header with a \emph{white hole} header that
indicates the object is locked.  If another thread enters the closure,
they will spinlock until the original header is restored.  A spinlock is
used as the critical regions they protect tend to be very short, and it would
be expensive to allocate a mutex for every closure that needed one.

We can now describe how MVars and asynchronous exceptions are
synchronized.  An MVar uses a white hole on the MVar itself to protect
manipulations of the blocked thread queue; additionally, when it needs
to wakeup a thread, it may need to send a message to the capability
which owns the unblocked thread.  An asynchronous exception is even
simpler: it is simply a message to the capability which owns the thread.

\subsection{Summary}

Haskell \emph{threads} are lightweight threads of execution which
multiplex onto multiple CPU cores.  Each core has a \emph{Haskell
execution context} which contains a scheduler for running these threads;
in order to handle FFI calls execution contexts can migrate from core to
core as necessary.  Threads are load balanced across execution contexts
by having execution contexts with work push threads to contexts which
don't.  Sparks are a simple way of utilizing idle cores when there is no
other real work to do.

By in large, all inter-thread communication in Haskell is explicit, thus
making it much easier to compile Haskell in a thread-safe way.
\emph{MVars}, \emph{asynchronous exceptions} and \emph{STM} are
explicitly used by Haskell code and can be efficiently implemented by
taking advantage of our representation of Haskell threads.  The basic
techniques by which these are synchronized are \emph{messages} and
\emph{white holes} (spinlocks).  We defer the issue of synchronizing
lazy evaluation to the next section.

\subsection{Further reading}

We have said little about how to use the concurrency mechanisms described here. \XXX
