\section{Concurrency and parallelism}

We now turn our attention to the implementation of
concurrency~\cite{PeytonJones:1996:CH:237721.237794} and
parallelism~\cite{Harris:2005:HSM:1088348.1088354} in the GHC runtime.
It is well worth noting the difference between concurrency and
parallelism: a parallel program uses multiple processing cores in order
to speed up computation, while a concurrent program simply involves
multiple threads of control, which notionally execute ``at the same
time'' but may be implemented merely by interleaving their execution.

GHC is both concurrent and parallel, but many of the features we
describe are applicable in non-parallel but concurrent systems (e.g.
systems which employ cooperative threading on one core): indeed, some
were developed before GHC had a shared memory multi-processor
implementation.  Thus, in the first section, we consider how to
implement \emph{threads} without relying on hardware support.  We then
describe a number of inter-thread communication mechanisms which are
useful in concurrent programs (and say how to synchronize them). Finally
describe what needs to be done to make \emph{compiled} code thread-safe.

\subsection{Concurrency}

Concurrent Haskell~\cite{PeytonJones:1996:CH:237721.237794} exposes the
abstraction of a \emph{Haskell thread} to a programmer. As the
operating system also provides native threads, one may wonder if there
any difference between a Haskell thread and an OS thread.

Many languages with multithreading support simply expose OS threads: a
``thread'' is one and the same as an OS thread.  While simple, this
approach has costs.  In particular, users of these languages must be
economical in their use of threads, as most operating systems cannot
support thousands of simultaneous OS threads.  This is a shame, because
in many applications the most natural way to \emph{structure} a program
involves thousands of logical threads: consider a web server, for
example, which logically has a thread of execution per request.
Furthermore, when a program cannot be made thread-safe and must be run
in a single OS thread, no concurrency is available; it must be
implemented in userspace, as is the case with many asynchronous IO
libraries.

To support cheap threads, we must \emph{multiplex} multiple Haskell
threads onto a single OS thread.  A Haskell thread runs until it is
\emph{preempted}, at which point we suspend its execution and switch to
running another thread, an operation handled by the \emph{scheduler}.
How is this preemption implemented?  True preemption is difficult to
implement, because it implies we can interrupt the execution of code any
any point, even if it would be leaving some data structures in an
intermediate state.  So instead, compiled Haskell code yields
cooperatively\footnote{Cooperative in the sense that the compiled code
has explicit yield points, not in the sense that the code a programmer
has to write contains yield points.} at various safe points, where we
know that the heap is in order and our internal state is saved (to be
restored on resumption).  These points are in fact when Haskell code
performs a \emph{heap check} to find out if there is enough free memory
on the heap to perform an allocation.  These checks automatically are
safe, because Haskell code already needs to be able to yield to the
garbage collector, in case we have run out of memory.  This check is
extended to also yield when a thread has been preempted.\footnote{ In
practice, preemption is handled by way of a timer signal, which, when
fired, sets the heap limit to zero, triggering a faux ``heap overflow''
which the scheduler can then identify as a preempt and not a true
request for garbage collection.  Thus, the heap check and preemption
check is a single conditional branch.}

Once we have a way of suspending and resuming threads, the scheduler
loop is quite simple. Maintain a \emph{run queue} of threads, and repeatedly:

\begin{enumerate}
    \item Pop the next thread off of the run queue,
    \item Run the thread until it yields,
    \item Check why the thread exited:
        \begin{itemize}
            \item If it ran out of heap, call the GC and then run the
                thread again;
            \item If it ran out of stack, enlarge the stack and then run
                the thread again;
            \item If it was preempted, push the thread to the end of the queue;
            \item If the thread exited, continue.
        \end{itemize}
\end{enumerate}

\subsection{Multiprocessor execution}

Generalizing the system to allow multiple Haskell threads to run truly
simultaneously on multiple processors requires a fundamental
reassessment of the execution platform.  We want to retain the concept
of ligthweight threads, but allow them to run simultaneously on
multiple processor cores.

We therefore introduce the concept of a \emph{Haskell Execution
  Context} (HEC).\footnote{In the implementation we use the term
  ``Capability'' for historical reasons.}  The HEC represents the
resources necessary for running Haskell code, and the permission to do
so; a HEC can be owned by only one OS thread at a time, and that OS
thread uses the HEC to run Haskell code.  The HEC is like a
\emph{virtual processor}; we typically have one HEC per real processor
core in the machine.

For each piece of state that the running Haskell code needs access to,
we must decide whether the data structure is \emph{shared} by the
HECs, or if it is \emph{local}, so that each HEC maintains its
own copy or portion of the data.  Access to any shared data must of
course be appropriately synchronized.  Wherever possible, we prefer
local data, because it can be accessed without synchronization or
communication between processors.  Specifically, each HEC contains the
following local data:

\begin{itemize}
\item A run queue of Haskell threads.  Threads always run on the same
  HEC unless they are \emph{migrated}, which may happen as a result of
  automatic load balancing (Section \ref{sec:load-balancing}).
\item ToDo: more here ...
\end{itemize}

In many ways a HEC is like a mini-RTS, running a scheduler and
managing its own queue of Haskell threads.  However, there is one
significant piece of state that is shared between all the HECs: the
heap.  We have no option but to make the heap shared, because
Concurrent Haskell is designed around a shared data model, in which
data can be implicitly shared between threads by virtue of being
shared at the time the thread was forked, or subsequently explicitly
shared via one of the synchronisation mechanisms.  Furthermore, we
wish to use parallel programming models that depend on implicit
sharing, such as sparks (Section \ref{sec:sparks}). (Languages such as
Erlang, that are based on a concurrency programming model in which
data is moved between processes explicitly, support an architecture of
separate per-process heaps.)

With the heap being shared between multiple running Haskell threads,
we have to arrange that simultaneous access to heap objects is safe;
we'll consider the issues around this later in
Section~\ref{sec:parallel-heap}.

Even though the heap is shared, it is crucial that each HEC be able to
allocate memory from the heap without synchronisation.  Allocation
happens rapidly in Haskell (1 GB per second is typical on a single
core), so adding synchronisation to the fast path would destroy
performance.  Hence, each HEC has its own private \emph{nursery} into
which it can allocate. The garbage collector is tuned for recovering
garbage in the nurseries quickly, the idea being that the nursery can
fit in the cache, and we recycle it very quickly so that much of the
garbage never even hits the main memory.  We describe this in more
detail in Section~\ref{parallel-gc}.

\subsection{Communication between HECs}

While we strive to avoid any unnecessary communication between HECs in
order to maximise parallel performance, there are situations where
HECs must talk to each other.  These are:

\begin{itemize}
\item When a thread running on one HEC causes a thread on another HEC
  to become \emph{unblocked}.
\item When a thread running on one HEC encounters a \emph{black hole}
  owned by a thread running on another HEC
  (Section~\ref{sec:black-holes}).
\item When a thread running on one HEC throws an asynchronous
  exception to a thread running on another HEC
  (Section~\ref{sec:asynchronous-exceptions}).
\item When a HEC exhausts its nursery and needs to perform a garbage
  collection (Section~\ref{sec:parallel-gc}).
\item When the system is shutting down.
\end{itemize}

Except for the garbage collection and shutting down cases, these
communications are all handled by \emph{message passing}.  The
initiating HEC creates a message object in the heap, and adds it to
the message queue of the receiving HEC (the message queue of a HEC is
protected by a mutex).  It then interrupts the receiving HEC using
software polling (Section~\ref{hplim-zero-trick}).  The receiving HEC
collects the message from its message queue the next time it runs the
scheduler loop, and takes whatever action is necessary, possibly
responding with a reply of some kind.

Message passing turns out to be an ideal technique in these situations
for the following reasons:

\begin{itemize}
\item The message send is asynchronous; the current HEC can get on
  with other work while it waits for a response.  In the cases where a
  local thread is blocked waiting for something to happen on another
  HEC, the current HEC just takes another thread from its thread queue
  and continues working.

\item Message passing allows us to declare certain pieces of state as
  private to a particular HEC, which simplifies synchronization.  For
  instance, threads themselves are heap objects (known as Thread State
  Objects, or TSOs), but only the HEC that owns the thread may modify
  its TSO.  For example, to unblock a thread that belongs to another
  HEC, we send a message to the HEC that owns it, and that HEC will
  modify the state of the thread appropriately and add it to the
  queue.  Or it might find that the thread has already been unblocked
  by another HEC---these race conditions are nicely solved by having a
  single owner for the thread.
\end{itemize}

In the past the GHC runtime tried to use various locking and
synchronization techniques to implement inter-HEC communication, but
it was devilishly difficult to get right.  Switching to
message-passing was a dramatic simplification.

We mentioned earlier that garbage collection is the exception to this:
in fact to perform a garbage collection we must interrupt all the HECs
as fast as possible so they can perform the GC together.  This is done
with software polling (Section~\ref{hplim-zero-trick}), and a global
state to indicate that a GC is pending.  The global state itself is
modified atomically, just in case two HECs happen to request a GC at
the same time.

\subsection{Concurrency and foreign function calls}

The benefit of lightweight concurrency is that it offers a way of
producing threads of control which are indistinguishable from OS
threads, but are dramatically cheaper.  However, there are some places
when this illusion does not hold: of particular note is the
\emph{foreign function interface} (FFI)~\cite{Marlow04extendingthe},
which permits Haskell code to call out to foreign code not compiled by
GHC, and vice versa.  What happens if an FFI call blocks?  As our
concurrency is cooperative, if an FFI call refuses to return to the
scheduler, the execution of all other threads will grind to a halt.
There isn't really any way around this problem without introducing true
OS threads to the mix: what we'd like to do is arrange for the blocking
FFI call and the scheduler to run on different OS threads concurrently.

With multiple OS threads running, we have to arrange that only one of
the OS threads is running Haskell code at a given time (we'll relax
this restriction in the next section).  Thus we introduce the concept
of a \emph{Haskell Execution Context} (HEC), which is a data structure
containing the resources necessary for running Haskell code---the
heap, the run queue of threads, and so on.\footnote{In the
  implementation we use the term ``Capability'' for historical
  reasons.} The HEC is owned by a single OS thread at any given time,
and the owning OS thread runs the scheduler, which runs Haskell
threads from the run queue.  If a Haskell thread makes a foreign call,
the OS thread running it \emph{releases} the HEC before making the
call, and ensures that there is another OS thread ready to pick up the
HEC and continue to run the other Haskell threads while the foreign
call is in progress.  The HEC contains a pool of OS threads for this
purpose, and more are created on demand.

When an FFI call returns, we'd like to return the HEC to the
original OS thread. Thus, we have to modify the scheduler loop as follows:

\begin{enumerate}
    \item Check if we need to yield the HEC to some other OS thread, e.g. if an FFI call has just finished,
    \item \emph{Run as before.}
\end{enumerate}

\subsection{Bound Threads}

\SM{I think we could move this later.}

Another place where Haskell threads differ from OS threads is thread
local state.  As HECs are passed between OS threads, we make no
guarantee that any given FFI call will be performed on the same OS
thread as the previous FFI call.  To accomodate Haskell threads which
rely on thread-local state, GHC introduces the notion of a \emph{bound thread},
which binds a Haskell thread to a fresh OS thread.\footnote{GHC
    also calls these \emph{in-calls}, due to the fact that external code
    which calls into Haskell must be bound: if it makes the Haskell code
calls out via the FFI again, the inner and outer C code may rely on the
same thread local state.}

How can we support bound threads?  A simple scheme is to give each bound
thread its own OS thread.  However, if we have only one HEC, we
need to coordinate these new OS threads so that only one is running
Haskell code at a time.  We can do this by, once again, passing the
HEC to whichever OS thread truly needs to run the bound thread:

\begin{enumerate}
    \item \emph{Check if we need to yield the HEC to some other OS thread, e.g. if an FFI call has just finished,}
    \item \emph{Pop the next thread off of the run queue,}
    \item Check if the thread is bound:
        \begin{itemize}
            \item If the thread is bound but is already scheduled on the OS thread, proceed.
            \item If the thread is bound but on the wrong OS thread, give the HEC to the correct task.
            \item If the thread is not bound but this OS thread is bound, give up the HEC, so that any HEC that truly needs this OS thread will be able to get it.
        \end{itemize}
    \item \emph{Run as before.}
\end{enumerate}

While the movement of capabilities from task to task is somewhat intricate,
it imposes no overhead when bound threads are not used.

\subsection{Multiprocessor execution}

Assuming that the compiled Haskell code is thread safe~(see
Section~\ref{sec:sync}), it is now very simple to parallelize execution:
allocate multiple HECs!  Each OS thread in possession of a
HEC runs the scheduler loop, and everything works the way you'd
expect.

There is one primary design choice: should each HEC have its own
run queue, or should there be a single global run queue?  A global run
queue avoids the need for any sort of load balancing, but requires
synchronization and makes it difficult to keep Haskell threads running on
the same core, destroying data locality.  With separate run queues, however,
threads must be load balanced: one HEC could accumulate too many
threads while the other HECs idle.

The very simple load balancing scheme GHC uses is as follows: when a
HEC runs out of threads to run, it suspends itself (releasing its
lock) and waits on a condition variable.  When a HEC has too many
threads to run (it checks each iteration of its schedule loop), it takes
out locks on as many idle HECs as it can and pushes its excess
threads onto their run queues.  Finally, it releases the locks and
signals on each idle HECs that they can start running.  The
benefit of this scheme is that the run queues are kept completely
unsynchronized, but a plausible alternative is to use work-stealing
queues.

\subsection{Sparks}
\label{sec:sparks}

\emph{Sparks}~\cite{Marlow2009} are a mechanism for speculative parallel computation.
When a program is not utilizing all of its CPUs, the other CPUs can be
reallocated to evaluate thunks that the programmer indicated are likely
to be needed in the future.  These units of work are called sparks, and
they offer a mechanism for cheap, deterministic parallelism---in contrast to
Haskell threads, which are more expensive and nondeterministic.

Sparks take advantage of Haskell's lazy evaluation (Section~\ref{sec:lazy}) to provide a source
of units of work for sparking.  The ability to speculatively evaluate
thunks---a spark is not guaranteed to be evaluated---comes from the fact
that thunks encapsulate pure code and have no side effects.
Furthermore, as thunk update is thread safe (Section~\ref{sec:sync}),
they provide a natural mechanism of communicating the result of a
computation back to the thread that requested it.

Given thread-safe thunks, the implementation of sparks is quite simple:
when a computation is sparked, it is stored in a \emph{spark pool}
associated with a HEC.  When a HEC has no work to do (e.g.
its run queue is empty), it creates a \emph{spark thread}, which
repeatedly attempts to retrieve a spark from the HEC's spark pool
and evaluate it.  Thunk update ensures the results get propagated back
to the main thread. If the HEC receives any real work, it
immediately terminates the spark thread.

Whereas threads rarely need to be load balanced, sparks frequently need
to be migrated, as the HEC that is generating sparks is likely to
be the one that is also doing real work.  Sparks are balanced using
bounded work-stealing queues~\cite{Arora:1998:TSM:277651.277678,Hendler2005}, where a spark thread goes and steals
sparks from other threads when it has none to execute.

One important optimization for sparks is removing sparks which will not
contribute any useful parallelism.  For example, if the spark is
evaluated in the main thread before a spark thread gets around to it,
the spark is \emph{fizzled} and should be removed from the spark pool.
Additionally, if a spark's thunk has no references to it (i.e. is dead),
then the result cannot possibly have an impact on program execution and
it should also be pruned.  It is relatively easy to check for both of
these conditions in the garbage collector, by traversing the spark pool
and checking if the spark points to a thunk that was successfully evacuated.\footnote{An alternate design is to have sparks be GC roots, so that an outstanding spark keeps its data alive. While this is convenient for the implementation of \emph{parallel strategies}, it can result in space leaks, and GHC no longer uses this strategy.}

\subsection{MVars}
\label{sec:MVar}

Haskell offers a variety of ways for Haskell threads to interact with
each other.  We now describe how to implement MVars, the simplest form
of synchronized communication available to Haskell threads.  An MVar is
a mutable location that may be empty.  There are two operations which
operate on an MVar: \verb|takeMVar|, which blocks until the location is
non-empty, then reads and returns the value, leaving the location empty,
and \verb|putMVar|, which dually blocks until the location is empty,
then writes its value into location.  An MVar can be thought of as a
lock when its contents are ignored.

The blocking behavior is the most interesting aspect of MVars:
ordinarily, one would have to implement this functionality using a
condition variable.  However, because our Haskell threads are not
operating system threads, we can do something much more lightweight:
when a thread realizes it needs to block, it simply adds itself to a
\emph{blocked threads queue} corresponding to the MVar.  When another
thread fills in the MVar, it can check if there is a thread on the
blocked list and wake it up immediately.

This scheme has a number of good properties.  First, it allows us
to implement efficient \emph{single wake-up} on MVars, where only one of
the blocking threads is permitted to proceed. Second, using a FIFO
queue, we can offer a fairness guarantee, which is that no thread
remains blocked on an MVar indefinitely unless another thread holds the
MVar indefinitely.  Finally, because threads are garbage collected
objects, if the MVar a thread is blocking on becomes unreachable,
\emph{so does the thread.}  Thus, in some cases, we can tell when
a blocked thread is deadlocked and terminate it.

\subsection{Asynchronous exceptions}
\label{sec:asynchronous-exceptions}

MVars are a cooperative form of communication, where a thread must
explicitly opt-in to receive messages.  Asynchronous
exceptions~\cite{Marlow:2001:AEH:378795.378858}, on the other hand,
permit threads to induce an exception in another thread without its
cooperation.  Asynchronous exceptions are much more difficult to program
with than their synchronous brethren: as a signal can occur at any point
in a program's execution, the program must be careful to register
handlers which ensure that any resources are released and invariants are
preserved.  In \emph{pure} functional programs, this requirement is
easier to fulfill, as pure code can always be safely aborted.
Asynchronous exceptions are quite useful in a variety of situations,
including timing out long running computation, aborting speculative
computation and handling user interrupts.

Triggering an asynchronous exception is relatively simple with
preemptive scheduling: force the target thread back to the scheduler,
at which point the scheduler can introduce the exception and walk up the
stack, looking for exception handlers.  In
case a thread is operating in a sensitive region, an exception masking
flag can be set, which defers the delivery of the exception (it is saved
to a list of waiting exceptions on the thread itself).

There are two primary differences between how asynchronous exceptions
and normal exceptions are handled.  The first is that a thread which is
messaged may have been blocking on some other thread (i.e. on an MVar);
thus, when an asynchronous exception is received, the thread must remove
itself from the blocked list of threads.\footnote{If your queues are
    singly linked, you will need some cleverness to entries. GHC does
    this by stubbing out an entry with an indirection, the very same
that is used when a thunk is replaced with its true value, and modifying
queue handling code to skip over indirections; because blocking queues
live on the heap, the garbage collector will clean it up for us in the end.}

\Red{This explanation probably still needs a little work}

The second difference is how lazy evaluation is handled. When pure code
raises an exception, referential transparency demands that any other
execution of that code will result in the same exception.  Thus, while
we are walking up the stack, when we see an \emph{update frame}, which
is a continuation responsible for taking a value and saving it to the
thunk (overwriting it), we go ahead and instead overwrite the thunk with a new
thunk that always throws the exception.  However, in the case of an
asynchronous exception, the code could have simply been unlucky: when
someone else asks for the same computation, we should simply resume
where we left off.  Thus, we instead \emph{freeze} the state of
evaluation by saving the current stack into the thunk.~\cite{Reid1999}
This involves walking up the stack and performing the following
operations when we encounter an update frame:

\begin{enumerate}
    \item Allocate a new closure (called an \verb|AP_STACK| closure, for
        ``apply stack'') which contains the contents of the stack above
        the frame, and overwriting the old thunk\footnote{Possibly a
        black hole.} with a pointer to this closure,
    \item Truncate the stack up to and including the update frame, and
    \item Push a pointer to the new \verb|AP_STACK| closure onto the stack.
\end{enumerate}

The result is a chain of \verb|AP_STACK| closures, where the top of each
frozen stack links to the next frozen stack.  When another thread
evaluates an \verb|AP_STACK| closure (intending to evaluate the thunk),
it pushes the frozen stack onto the current stack, thus resuming the
computation.

\subsection{STM}

Software Transactional Memory, or STM, is an abstraction for concurrent
communication which emphasizes transactions as the basic unit of
communication.  The big benefit of STM over MVars is that they are
composable: while programs involving multiple MVars must be very careful
to avoid deadlock, programs using STM can be composed together
effortlessly.

Before discussing what is needed to support STM in the runtime system,
it is worth mentioning what we do not have to do.  In many languages,
an STM implementation must also manage all side-effects that any code
in a transaction may perform.  In an impure language, there may be many
of these side-effects (even if they are local), and the runtime must
make them transactional at great cost.  In Haskell, however, the type system
enforces that code running in an STM transaction will only ever perform
pure computation or explicit manipulation of shared state.  This eliminates
a large inefficiency that plagues many other STM implementation.

\Red{How is it implemented}

\Red{Maybe move this below messages and white holes}

\subsection{Messages and white holes}

In the descriptions above, we said very little about the synchronization
that is necessary to implement them in a multiprocessor environment.
Under the hood, the GHC runtime has two primary methods of synchronization:
\emph{messages} and \emph{white holes} (effectively a spinlock).  The
runtime makes very sparing use of OS level condition variables and
mutexes, since they tend to be expensive.

GHC uses a very simple message passing architecture to pass messages
between HECs.  A HEC sends a message by:

\begin{enumerate}
    \item Allocating a message object on the heap;
    \item Taking out a lock on the message inbox of the destination HEC;
    \item Appending the message onto the inbox;
    \item Interrupting the HEC, using the same mechanism as the context switch timer (setting the heap limit to zero); and
    \item Releasing the lock.
\end{enumerate}

This allows the message to be handled by the destination HEC at
its convenience, i.e. after the running Haskell code yields and we
return to the scheduling loop.  In general, the benefit of message
passing systems is that they remove the need for synchronizing any of
the non-local state that another HEC \emph{might} want to modify: instead,
the HEC just sends a message asking for the state to be modified.

When sending a message is not appropriate, e.g. in the case of
synchronized access to closures which are not owned by any HEC in
particular, GHC instead uses a very simple spinlock on the closure
\emph{header}, replacing the header with a \emph{white hole} header that
indicates the object is locked.  If another thread enters the closure,
they will spinlock until the original header is restored.  A spinlock is
used as the critical regions they protect tend to be very short, and it would
be expensive to allocate a mutex for every closure that needed one.

We can now describe how MVars and asynchronous exceptions are
synchronized.  An MVar uses a white hole on the MVar itself to protect
manipulations of the blocked thread queue; additionally, when it needs
to wakeup a thread, it may need to send a message to the HEC
which owns the unblocked thread.  An asynchronous exception is even
simpler: it is simply a message to the HEC which owns the thread.

\subsection{Summary}

Haskell \emph{threads} are lightweight threads of execution which
multiplex onto multiple CPU cores.  Each core has a \emph{Haskell
execution context} which contains a scheduler for running these threads;
in order to handle FFI calls execution contexts can migrate from core to
core as necessary.  Threads are load balanced across execution contexts
by having execution contexts with work push threads to contexts which
don't.  Sparks are a simple way of utilizing idle cores when there is no
other real work to do.

By in large, all inter-thread communication in Haskell is explicit, thus
making it much easier to compile Haskell in a thread-safe way.
\emph{MVars}, \emph{asynchronous exceptions} and \emph{STM} are
explicitly used by Haskell code and can be efficiently implemented by
taking advantage of our representation of Haskell threads.  The basic
techniques by which these are synchronized are \emph{messages} and
\emph{white holes} (spinlocks).  We defer the issue of synchronizing
lazy evaluation to the next section.

\subsection{Further reading}

We have said little about how to use the concurrency mechanisms described here. \XXX
