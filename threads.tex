\section{Concurrency and parallelism}

We now turn our attention to the implementation of
concurrency~\cite{PeytonJones:1996:CH:237721.237794} and
parallelism~\cite{Harris:2005:HSM:1088348.1088354} in the GHC runtime.
It is well worth noting the difference between concurrency and
parallelism: a parallel program uses multiple processing cores in order
to speed up computation, while a concurrent program simply involves
multiple threads of control, which notionally execute ``at the same
time'' but may be implemented merely by interleaving their execution.

GHC is both concurrent and parallel, but many of the features we
describe are applicable in non-parallel but concurrent systems (e.g.
systems which employ cooperative threading on one core): indeed, some
were developed before GHC had a shared memory multi-processor
implementation.  Thus, in the first section, we consider how to
implement \emph{threads} without relying on hardware support.  We then
describe a number of inter-thread communication mechanisms which are
useful in concurrent programs (and say how to synchronize them). Finally
describe what needs to be done to make \emph{compiled} code thread-safe.

\subsection{Threads}

Concurrent Haskell~\cite{PeytonJones:1996:CH:237721.237794} exposes the
abstraction of a \emph{Haskell thread} to a programmer. As the
operating system also provides native threads, one may wonder if there
any difference between a Haskell thread and an OS thread.

Many languages with multithreading support simply expose OS threads: a
``thread'' is one and the same as an OS thread.  While simple, this
approach has costs.  In particular, users of these languages must be
economical in their use of threads, as most operating systems cannot
support thousands of simultaneous OS threads.  This is a shame, because
in many applications the most natural way to \emph{structure} a program
involves thousands of logical threads: consider a web server, for
example, which logically has a thread of execution per request.
Furthermore, when a program cannot be made thread-safe and must be run
in a single OS thread, no concurrency is available; it must be
implemented in userspace, as is the case with many asynchronous IO
libraries.

To support cheap threads, we must \emph{multiplex} multiple Haskell
threads onto a single OS thread.  A Haskell thread runs until it is
\emph{preempted}, at which point we suspend its execution and switch to
running another thread, an operation handled by the \emph{scheduler}.
How is this preemption implemented?  True preemption is difficult to
implement, because it implies we can interrupt the execution of code any
any point, even if it would be leaving some data structures in an
intermediate state.  So instead, compiled Haskell code yields
cooperatively\footnote{Cooperative in the sense that the compiled code
has explicit yield points, not in the sense that the code a programmer
has to write contains yield points.} at various safe points, where we
know that the heap is in order and our internal state is saved (to be
restored on resumption).  These points are in fact when Haskell code
performs a \emph{heap check} to find out if there is enough free memory
on the heap to perform an allocation.  These checks automatically are
safe, because Haskell code already needs to be able to yield to the
garbage collector, in case we have run out of memory.  This check is
extended to also yield when a thread has been preempted.\footnote{ In
practice, preemption is handled by way of a timer signal, which, when
fired, sets the heap limit to zero, triggering a faux ``heap overflow''
which the scheduler can then identify as a preempt and not a true
request for garbage collection.  Thus, the heap check and preemption
check is a single conditional branch.}

Once we have a way of suspending and resuming threads, the scheduler
loop is quite simple. Maintain a \emph{run queue} of threads, and repeatedly:

\begin{enumerate}
    \item Pop the next thread off of the run queue,
    \item Run the thread until it yields,
    \item Check why the thread exited:
        \begin{itemize}
            \item If it ran out of heap, call the GC and then run the
                thread again;
            \item If it ran out of stack, enlarge the stack and then run
                the thread again;
            \item If it was preempted, push the thread to the end of the queue;
            \item If the thread exited, continue.
        \end{itemize}
\end{enumerate}

\subsection{Foreign function interface}

The benefit of lightweight concurrency is that it offers a way of
producing threads of control which are indistinguishable from OS
threads, but are dramatically cheaper.  However, there are some places
when this illusion does not hold: of particular note is the
\emph{foreign function interface} (FFI)~\cite{Marlow04extendingthe},
which permits Haskell code to call out to foreign code not compiled by
GHC, and vice versa.  What happens if an FFI call blocks?  As our
concurrency is cooperative, if an FFI call refuses to return to the
scheduler, the execution of all other threads will grind to a halt.
There isn't really any way around this problem without introducing true
OS threads to the mix: what we'd like to do is arrange for the blocking
FFI call and the scheduler to run on different OS threads concurrently.

As it turns out, it is simpler to move the scheduler to a different OS
thread than the FFI. Thus, we decompose OS threads into two parts: the
OS thread itself (called a \emph{task} in GHC terminology), and the
Haskell execution context (called a \emph{capability}).  The Haskell
execution context contains the scheduler loop and is responsible for the
contents of the run queue: when executing, it is owned exclusively by
the particular task (OS thread) which is running it.  A single-threaded
Haskell program has one capability: the capability is a \emph{global
lock} on the Haskell runtime.  Now, before a blocking FFI call is made,
the task releases the capability: if there is another idle \emph{worker
thread}, it can acquire the now free capability and continue running
Haskell code.

When an FFI call returns, we'd like to return the capability to the
original OS thread. Thus, we have to modify the scheduler loop as follows:

\begin{enumerate}
    \item Check if we need to yield the capability to some other OS thread, e.g. if an FFI call has just finished,
    \item \emph{Run as before.}
\end{enumerate}

Another place where Haskell threads differ from OS threads is thread
local state.  As capabilities are passed around OS threads, we make no
guarantee that any given FFI call will be performed on the same OS
thread as the previous FFI call.  To accomodate Haskell threads which
rely on thread-local state, Haskell introduces a \emph{bound thread},
which binds a Haskell thread to a fresh OS thread.\footnote{GHC
    also calls these \emph{in-calls}, due to the fact that external code
    which calls into Haskell must be bound: if it makes the Haskell code
calls out via the FFI again, the inner and outer C code may rely on the
same thread local state.}

How can we support bound threads?  A simple scheme is to give each bound
thread its own OS thread.  However, if we have only one capability, we
need to coordinate these new OS threads so that only one is running
Haskell code at a time.  We can do this by, once again, passing the
capability to whichever OS thread truly needs to run the bound thread:

\begin{enumerate}
    \item \emph{Check if we need to yield the capability to some other OS thread, e.g. if an FFI call has just finished,}
    \item \emph{Pop the next thread off of the run queue,}
    \item Check if the thread is bound:
        \begin{itemize}
            \item If the thread is bound but is already scheduled on the OS thread, proceed.
            \item If the thread is bound but on the wrong OS thread, give the capability to the correct task.
            \item If the thread is not bound but this OS thread is bound, give up the capability, so that any capability that truly needs this OS thread will be able to get it.
        \end{itemize}
    \item \emph{Run as before.}
\end{enumerate}

While the movement of capabilities from task to task is somewhat intricate,
it imposes no overhead when bound threads are not used.

\subsection{Load balancing}

Assuming that the compiled Haskell code is thread safe~(see
Section~\ref{sec:sync}), it is now very simple to parallelize execution:
allocate multiple capabilities!  Each OS thread in possession of a
capability runs the scheduler loop, and everything works the way you'd
expect.

There is one primary design choice: should each capability have its own
run queue, or should there be a single global run queue?  A global run
queue avoids the need for any sort of load balancing, but requires
synchronization and makes it difficult to keep Haskell threads running on
the same core, destroying data locality.  With separate run queues, however,
threads must be load balanced: one capability could accumulate too many
threads while the other capabilities idle.

The very simple load balancing scheme GHC uses is as follows: when a
capability runs out of threads to run, it suspends itself (releasing its
lock) and waits on a condition variable.  When a capability has too many
threads to run (it checks each iteration of its schedule loop), it takes
out locks on as many idle capabilities as it can and pushes its excess
threads onto their run queues.  Finally, it releases the locks and
signals on each idle capabilities that they can start running.  The
benefit of this scheme is that the run queues are kept completely
unsynchronized, but a plausible alternative is to use work-stealing
queues.

\subsection{Sparks}

\emph{Sparks}~\cite{Marlow2009} are a mechanism for speculative parallel computation.
When a program is not utilizing all of its CPUs, the other CPUs can be
reallocated to evaluate thunks that the programmer indicated are likely
to be needed in the future.  These units of work are called sparks, and
they offer a mechanism for cheap, deterministic parallelism---in contrast to
Haskell threads, which are more expensive and nondeterministic.

The ability to speculatively evaluate thunks---a spark is not guaranteed
to be evaluated---comes from the fact that thunks encapsulate pure code
and have no side effects.  Furthermore, as thunk update is thread safe
(Section~\ref{sec:sync}), they provide a natural mechanism of communicating
the result of a computation back to the thread that requested it.

Given thread-safe thunks, the implementation of sparks is quite simple:
when a computation is sparked, it is stored in a \emph{spark pool}
associated with a capability.  When a capability has no work to do (e.g.
its run queue is empty), it creates a \emph{spark thread}, which
repeatedly attempts to retrieve a spark from the capability's spark pool
and evaluate it.  Thunk update ensures the results get propagated back
to the main thread. If the capability receives any real work, it
immediately terminates the spark thread.

Whereas threads rarely need to be load balanced, sparks frequently need
to be migrated, as the capability that is generating sparks is likely to
be the one that is also doing real work.  Sparks are balanced using
bounded work-stealing queues~\cite{Arora:1998:TSM:277651.277678,Hendler2005}, where a spark thread goes and steals
sparks from other threads when it has none to execute.

One important optimization for sparks is removing sparks which will not
contribute any useful parallelism.  For example, if the spark is
evaluated in the main thread before a spark thread gets around to it,
the spark is \emph{fizzled} and should be removed from the spark pool.
Additionally, if a spark's thunk has no references to it (i.e. is dead),
then the result cannot possibly have an impact on program execution and
it should also be pruned.  It is relatively easy to check for both of
these conditions in the garbage collector, by traversing the spark pool
and checking if the spark points to a thunk that was successfully evacuated.\footnote{An alternate design is to have sparks be GC roots, so that an outstanding spark keeps its data alive. While this is convenient for the implementation of \emph{parallel strategies}, it can result in space leaks, and GHC no longer uses this strategy.}

\subsection{MVars}

Haskell offers a variety of ways for Haskell threads to interact with
each other (MVars, asynchronous exceptions, STM, even lazy evaluation).
We now describe how to implement MVars, the simplest form of
synchronized communication available to Haskell threads.  An MVar is a mutable
location that may be empty.  There are two operations which operate on
an MVar: \verb|takeMVar|, which blocks until the location is non-empty,
then reads and returns the value, leaving the location empty, and
\verb|putMVar|, which dually blocks until the location is empty, then
writes its value into location.  An MVar can be thought of as a lock when
its contents are ignored.

The blocking behavior is the most interesting aspect of MVars:
ordinarily, one would have to implement this functionality using a
condition variable.  However, because our Haskell threads are not
operating system threads, we can do something much more lightweight:
when a thread realizes it needs to block, it simply adds itself to a
\emph{blocked threads queue} corresponding to the MVar.  When another
thread fills in the MVar, it can check if there is a thread on the
blocked list and wake it up immediately.

This scheme has a number of good properties.  First, it allows us
to implement efficient \emph{single wake-up} on MVars, where only one of
the blocking threads is permitted to proceed. Second, using a FIFO
queue, we can offer a fairness guarantee, which is that no thread
remains blocked on an MVar indefinitely unless another thread holds the
MVar indefinitely.  Finally, because threads are garbage collected
objects, if the MVar a thread is blocking on becomes unreachable,
\emph{so does the thread.}  Thus, in some cases, we can tell when
a blocked thread is deadlocked and terminate it.

\subsection{Asynchronous exceptions}

MVars are a cooperative form of communication, where a thread must
explicitly opt-in to receive messages.  Asynchronous
exceptions~\cite{Marlow:2001:AEH:378795.378858}, on the other hand,
permit threads to induce an exception in another thread
without its cooperation.  Asynchronous exceptions are much more
difficult to program with than their synchronous brethren: as a signal can
occur at any point in a program's execution, the program must be careful
to register handlers which ensure that any resources are released and
invariants are preserved.  In \emph{pure} functional programs, this requirement
is easier to fulfill, as pure code can always be safely aborted.  When
asynchronous exceptions are available, however, they are useful
in a variety of cases, including timing out long running computation,
aborting speculative computation and handling user interrupts.

Triggering an asynchronous exception is relatively simple with
preemptive scheduling: preempt the target thread back to the scheduler,
at which point the scheduler can introduce the exception and walk up the
stack, much in the same way a normal exception would be handled.  In
case a thread is operating in a sensitive region, an exception masking
flag can be set, which defers the delivery of the exception (it is saved
to a list of waiting exceptions on the thread itself).

There are two primary differences between how asynchronous exceptions
and normal exceptions are handled.  The first is that a thread which is
messaged may have been blocking on some other thread (i.e. on an MVar);
thus, when an asynchronous exception is received, the thread must remove
itself from the blocked list of threads.\footnote{If your queues are
    singly linked, you will need some cleverness to entries. GHC does
    this by stubbing out an entry with an indirection, the very same
that is used when a thunk is replaced with its true value, and modifying
queue handling code to skip over indirections; because blocking queues
live on the heap, the garbage collector will clean it up for us in the end.}

The second difference is how lazy evaluation is handled. When pure code
raises an exception, referential transparency demands that any other
execution of that code will result in the same exception.  Thus, while
we are walking up the stack, when we see update frames \Red{link back},
we replace the thunk it would have updated with a new thunk that always
the exception.  However, in the case of an asynchronous exception, the
code could have simply been unlucky: when someone else asks for the same
computation, we should simply resume where we left off.  Thus, we
instead \emph{freeze} the state of evaluation by saving the current
stack into the thunk.~\cite{Reid1999}  This involves walking up the stack
and perform the following operations when we encounter an update frame:

\begin{enumerate}
    \item Allocating a new \verb|AP_STACK| closure which contains the
        contents of the stack above the frame, and overwriting the
        old thunk\footnote{Possibly a black hole.} with a pointer to this closure,
    \item Truncate the stack up to and including the update frame, and
    \item Pushing a pointer to the new \verb|AP_STACK| closure to the stack.
\end{enumerate}

When another thread evaluates one of these suspended thunks,
\verb|AP_STACK| pushes the frozen stack onto the current stack, thus
resuming the computation.  The top of the new stack may point to
yet another suspended thunk, thus repeating the process until we
have reached the point of the original execution. \Red{This is probably not understandable}

\subsection{STM}

Software Transactional Memory, or STM, is an abstraction for concurrent
communication which emphasizes transactions as the basic unit of
communication.  The big benefit of STM over MVars is that they are
composable: while programs involving multiple MVars must be very careful
to avoid deadlock, programs using STM can be composed together
effortlessly.

Before discussing what is needed to support STM in the runtime system,
it is worth mentioning what we do not have to do.  In many languages,
an STM implementation must also manage all side-effects that any code
in a transaction may perform.  In an impure language, there may be many
of these side-effects (even if they are local), and the runtime must
make them transactional at great cost.  In Haskell, however, the type system
enforces that code running in an STM transaction will only ever perform
pure computation or explicit manipulation of shared state.  This eliminates
a large inefficiency that plagues many other STM implementation.

\Red{How is it implemented}

\Red{Maybe move this below messages and white holes}

\subsection{Messages and white holes}

In the descriptions above, we said very little about the synchronization
that is necessary to implement them in a multiprocessor environment.
Under the hood, the GHC runtime has two primary methods of synchronization:
\emph{messages} and \emph{white holes} (effectively a spinlock).  The
runtime makes very sparing use of OS level condition variables and
mutexes, since they tend to be expensive.

GHC uses a very simple message passing architecture to pass messages
between capabilities.  A capability sends a message by:

\begin{enumerate}
    \item Allocating a message object on the heap;
    \item Taking out a lock on the message inbox of the destination capability;
    \item Appending the message onto the inbox;
    \item Interrupting the capability, using the same mechanism as the context switch timer (setting the heap limit to zero); and
    \item Releasing the lock.
\end{enumerate}

This allows the message to be handled by the destination capability at
its convenience, i.e. after the running Haskell code yields and we
return to the scheduling loop.  In general, the benefit of message
passing systems is that they remove the need for synchronizing any of
the non-local state that another capability \emph{might} want to modify: instead,
the capability just sends a message asking for the state to be modified.

When sending a message is not appropriate, e.g. in the case of
synchronized access to closures which are not owned by any capability in
particular, GHC instead uses a very simple spinlock on the closure
\emph{header}, replacing the header with a \emph{white hole} header that
indicates the object is locked.  If another thread enters the closure,
they will spinlock until the original header is restored.  A spinlock is
used as the critical regions they protect tend to be very short, and it would
be expensive to allocate a mutex for every closure that needed one.

We can now describe how MVars and asynchronous exceptions are
synchronized.  An MVar uses a white hole on the MVar itself to protect
manipulations of the blocked thread queue; additionally, when it needs
to wakeup a thread, it may need to send a message to the capability
which owns the unblocked thread.  An asynchronous exception is even
simpler: it is simply a message to the capability which owns the thread.

\subsection{Synchronization and black holes} \label{sec:sync}

In previous sections, we asserted that execution of Haskell threads
could be made parallel, assuming that the compiled Haskell code was
thread-safe.  As any developer who has needed to write thread-safe code
can attest, this is a tall order!

Fortunately, much of this work is already done.  We have already seen various mechanisms
for explicit interthread communication, which are all designed with
concurrent execution in mind:  synchronizing them is a relatively simple
task.  Furthermore, the vast majority of Haskell code is pure, and needs no
synchronization.

However, there is one major feature we have to worry about: lazy
evaluation.  Recall that after a thunk has been evaluated, its value on
the heap must be replaced with the fully evaluated value. Multiple
threads could evaluate a thunk in parallel, so na\"ively, these writes
must be synchronized.  This is extremely costly: Haskell programs do a
lot of thunk updates!

Once again, our saving grace is purity: as thunks represent pure
computation, evaluating a thunk twice has no observable effect: both
evaluations are guaranteed to come up with the same result.  Thus, we
should be able to keep updates to thunk unsynchronized, at the cost of
occasional duplication of work when two threads race to evaluate the
same thunk.

There are two details we must attend to.  The first detail is the fact
that a race can still be harmful, if we need to update a thunk in
multiple steps and the intermediate states are not valid.  In the case
of a thunk update, we need to both update the header and write down a
pointer to the result; if we update the header first, then the
intermediate state is not well-formed (the result field is empty); on
the other hand, if we update the result field first, we might clobber
important information in the thunk.  Instead, we leave a blank word in
every thunk where the result can be written in non-destructively, after
which the header of the closure can be changed.\footnote{Appropriate
write barriers must be added to ensure the CPU does not reorder these
instructions.}

\begin{verbatim}
word    step 1   step 2   step 3
   0     THUNK    THUNK      IND \ valid closure
   1         -   result   result /
   2   payload  payload  payload <- payload is slop
\end{verbatim}

The second detail is that some thunks take a long time to evaluate: we'd
like to avoid duplicating them.  Instead, when we start evaluating them,
we'd like other threads to block on us for the result.  This is
precisely what a \emph{black hole} (Section~\ref{sec:blackhole}) is for!
A thread seeking to evaluate the thunk will instead enter the black
hole; in a concurrent setting, we can place the thread on the blocked
queue of the other thread.  It is somewhat difficult to put a blocked
queue on the thunk itself (due to the lack of synchronization); instead,
GHC uses a per-thread list of black hole blockers which is traversed
every time a thread finishes updating a thunk.

In our original discussion, black holes were \emph{eagerly} written to
thunks upon evaluation, with the optimization that this could be
\emph{lazily} deferred for later.  If a black hole is written eagerly,
it is on the fast path of thunk updates, and we cannot use
synchronization.  We call these \emph{eager black holes} (also known as
\emph{grey holes}), which do not guarantee exclusivity.  Lazy blackholing is done more infrequently (when a thread is preempted), and thus
we can afford to use a CAS to implement them.\footnote{While multiple
    threads may have eagerly blackholed a thunk, we guarantee only one
    thread has lazily blackholed it.  If a thunk \emph{must not} be
duplicated, it can achieve this by forcing all of its callers to perform
lazy blackholing
(\texttt{noDuplicate\#}).  \texttt{unsafePerformIO} uses precisely
this mechanism in order to avoid duplication of IO effects.}

The upshot is that GHC is able to implement lazy evaluation without any
synchronization for most thunk updates, applying synchronization
\emph{only} when it is specifically necessary. The cost of this scheme
is low: a single extra field in thunk and a (rare) duplication of work
upon a race.

\subsection{Summary}

Haskell \emph{threads} are lightweight threads of execution which
multiplex onto multiple CPU cores.  Each core has a \emph{Haskell
execution context} which contains a scheduler for running these threads;
in order to handle FFI calls execution contexts can migrate from core to
core as necessary.  Threads are load balanced across execution contexts
by having execution contexts with work push threads to contexts which
don't.  Sparks are a simple way of utilizing idle cores when there is no
other real work to do.

By in large, all inter-thread communication in Haskell is explicit, thus
making it much easier to compile Haskell in a thread-safe way.
\emph{MVars}, \emph{asynchronous exceptions} and \emph{STM} are
explicitly used by Haskell code and can be efficiently implemented by
taking advantage of our representation of Haskell threads.  The basic
techniques by which these are synchronized are \emph{messages} and
\emph{white holes} (spinlocks).  The only implicit inter-thread
communication that occurs is lazy evaluation.  However, due to purity,
we can largely avoid synchronizing thunk updates, utilizing \emph{eager
blackholing} to reduce duplicate work, and synchronized \emph{lazy
blackholing} when duplication is unacceptable.

\subsection{Further reading}

We have said little about how to use the concurrency mechanisms described here. \XXX
